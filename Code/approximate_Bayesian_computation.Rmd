---
title: "Approximate Bayesian Computation"
author: "Grace Smith-Vidaurre"
date: "September 23, 2020"
output: html_document
---

```{r setup, eval = TRUE, echo = FALSE}

knitr::opts_knit$set(root.dir = "/home/owner/Desktop/GitHub_repos/origins-selection")

```

Purpose: Approximate Bayesian Computation analyses using the multidimensional site frequency spectrum (mSFS), the delimitR package developed by Megan Smith, and fastsimcaol2 for simulating the mSFS under different demographic scenarios. Installed delimitR version 2.0.2 from GitHub: https://github.com/meganlsmith. Had to install dependencies abcrf, sqldf and reticulate. Also installed the package radiator to convert between genind object and VCF format, needed to use Python code in another GitHub repo to get observed SFS data.

Here, 6 models will be run per invasive population with sufficient samples: Uruguay (URY) origin, Northern Argentina (NAR) origin, admixed origin, and these same 3 models but with a longer bottleneck time that goes to the present. 

```{r echo = TRUE, eval = TRUE, message = FALSE}

rm(list = ls())

X <- c("tidyverse", "pbapply", "data.table", "adegenet", "openxlsx", "abcrf", "delimitR")
invisible(lapply(X, library, character.only = TRUE))

# Path to the metadata spreadsheet
xls_path <- "/home/owner/Desktop/MANUSCRIPTS/Origins_Selection/DATA/Metadata_Barcodes"

# Path to Stacks output, including the HWE filtered SNPs in Structure format 
res_path <- "/media/owner/MYIOPSITTA/R/Origins_Selection/stacks"

# Path where ABC files will be written
out_path <- "/media/owner/MYIOPSITTA/R/Origins_Selection/ABC"
 
# Path where population maps written
map_path <- "/media/owner/MYIOPSITTA/R/Origins_Selection/info"

gpath <- "/home/owner/Desktop/MANUSCRIPTS/Origins_Selection/GRAPHICS"
seed <- 401

```

Read in metadata.
```{r echo = TRUE, eval = TRUE}

meta_dats <- read.xlsx(file.path(xls_path, "Mymon_RADlibraries_2015_2019_CombinedMetadata.xlsx"))
glimpse(meta_dats)

```

Read in the dataset of pre-processed neutral SNPs from the merged dataset. The number of individuals and loci is documented in BayeScan_post-processing.Rmd.
```{r echo = TRUE, eval = TRUE}

file_nm <- "merged_HWE_missingData_filters_noPosControlDups_neutralSNPs.str"

neutral_snps <- read.structure(file.path(file.path(res_path, "merged"), file_nm), n.ind = 173, n.loc = 320, col.lab = 1, col.pop = 2, row.marknames = 1, onerowperind = FALSE, ask = FALSE, NA.char = "-9")
str(neutral_snps)

str(neutral_snps@tab)

```

# Making population maps to get VCF files via Stacks for ABC modelling

How many invasive populations have sufficient samples (> 4 individuals) for ABC modelling?
```{r echo = TRUE, eval = TRUE}

# Get sampling sites
sites <- sapply(1:nrow(neutral_snps@tab), function(i){
  meta_dats$Site_Code[grep(paste("^", dimnames(neutral_snps@tab)[[1]][i], "$", sep = ""), meta_dats$Sample_Name)]
})
head(sites)

# Get sampling regions
regions <- sapply(1:nrow(neutral_snps@tab), function(i){
  meta_dats$Region[grep(paste("^", dimnames(neutral_snps@tab)[[1]][i], "$", sep = ""), meta_dats$Sample_Name)]
})
head(regions)

# Get native or invasive range
ranges <- sapply(1:nrow(neutral_snps@tab), function(i){
  meta_dats$Population[grep(paste("^", dimnames(neutral_snps@tab)[[1]][i], "$", sep = ""), meta_dats$Sample_Name)]
})
head(ranges)

samps_df <- data.frame(indiv = dimnames(neutral_snps@tab)[[1]]) %>%
  dplyr::mutate(
    site = sites,
    region = regions,
    range = ranges
  )
glimpse(samps_df)

# Only SEVI has too few samples (1 bird) for modelling
samps_df %>%
  filter(range == "Invasive") %>%
  group_by(site) %>%
  dplyr::summarise(
    n_indivs = n_distinct(indiv) 
  )

# Remove the SEVI individual, and all SAR individuals
samps_df <- samps_df %>%
  filter(site != "SEVI") %>%
  filter(region != "Southern Argentina") %>%
  droplevels()
glimpse(samps_df)

samps_df %>%
  pull(region) %>%
  unique()

# Add a column indicating population label for ABC modelling
samps_ABC_df <- samps_df %>%
  dplyr::mutate(
    ABC_pop = region,
    ABC_pop = recode(ABC_pop,
      `Southwestern Uruguay` = "pop0",
      `South Central Uruguay` = "pop0",
      `Spain` = "pop1",
      `Northern United States` = "pop1",
      `Southern United States` = "pop1",
      `Northern Argentina` = "pop2"
    )
  )
glimpse(samps_ABC_df)

# Randomly select 7 birds per region of URY to make models less unbalanced (otherwise 14 NAR and 14 URY individuals per model)
set.seed(seed)
samps_ABC_df <- samps_ABC_df %>%
  filter(grepl("Uruguay", region)) %>%
  group_by(region) %>%
  nest() %>%
  ungroup() %>%
  dplyr::mutate(
    rsamp = map2(data, 7, sample_n, replace = FALSE)
  ) %>%
  select(-data) %>%
  unnest(rsamp) %>%
 # Randomly select 14 birds per FLOR and CNCT
  bind_rows(
    samps_ABC_df %>%
      filter(grepl("CNCT", site)) %>%
      group_by(site) %>%
      nest() %>%
      ungroup() %>%
      dplyr::mutate(
        rsamp = map2(data, 14, sample_n, replace = FALSE)
      ) %>%
      select(-data) %>%
      unnest(rsamp)
  ) %>%
  bind_rows(
    samps_ABC_df %>%
      filter(grepl("FLOR", site)) %>%
      group_by(site) %>%
      nest() %>%
      ungroup() %>%
      dplyr::mutate(
        rsamp = map2(data, 14, sample_n, replace = FALSE)
      ) %>%
      select(-data) %>%
      unnest(rsamp)
  ) %>%
  # Add back all other samples (not URY, FLOR, CNCT)
  bind_rows(
    samps_ABC_df %>%
    filter(!grepl("Uruguay", region) & !grepl("CNCT", site) & !grepl("FLOR", site))
  )

# glimpse(samps_ABC_df)
# View(samps_ABC_df)

# Looks good
samps_ABC_df %>%
  group_by(region, site) %>%
  dplyr::summarise(
    n_indivs = length(indiv)
  )

```

Make a population map per each of the remaining 9 invasive populations. This map will have URY, NAR and INV birds (per each invasive population), but not SAR (to avoid making models too complex). The population map should encode URY individuals as pop1, INV individuals as pop2, and NAR individuals as pop3.
```{r echo = TRUE, eval = TRUE}

# Iterate over invasive range sampling sites
sites <- samps_ABC_df %>%
  filter(range == "Invasive") %>%
  pull(site) %>%
  unique() %>%
  as.character()
sites 
length(sites)  

# x <- 1
# i <- 1
invisible(pblapply(1:length(sites), function(x){
  
  # Initialize file name
  file_nm <- file.path(map_path, paste("popmap_mergedSNPs_ABCmodelling_", sites[x], ".txt", sep = ""))

  # Remove previous versions
  file.remove(file.path(map_path, paste("popmap_mergedSNPs_ABCmodelling_", sites[x], ".txt", sep = "")))
  
  # Get individuals for the given iteration
  tmp_df <- samps_ABC_df %>%
    filter(range == "Native" | site == sites[x]) %>%
    droplevels()
  
  # Order by ABC population
  tmp_df <- tmp_df %>%
    arrange(-desc(ABC_pop))
  # glimpse(tmp_df)
  # View(tmp_df)
    
  # Get individuals
  indivs <- tmp_df %>%
    pull(indiv) %>%
    as.character()
  
  # Get ABC populations
  ABC_pops <- tmp_df %>%
    pull(ABC_pop) %>%
    as.character()
  
  # Iterate over individuals to write out lines to this file
  pblapply(1:length(indivs), function(i){
  
    # Initialize the ABC population for the given sample
    reg <- ABC_pops[i]
  
    # Initialize the suffix to go after the sample name
    # Use ".1" for the PE reads here, since I used only the forward reads when merging SE and PE libraries
    suff <- ifelse(grepl("^SE_", indivs[i]), ".fil.sorted", ".1.sorted")
  
    # If not on the last individual, write out a new line symbol to start the next sample on a new line
    # No suffix after the sample name, to allow Stacks to recognize the paired-end file suffix after kmer_filter (.1.1.fil.fq and .2.2.fil.fq)
    if(i != length(indivs)){
      tmp_line <- paste(paste(paste(indivs[i], suff, sep = ""), reg, sep = "\t"), "\n", sep = "")
    } else {
      tmp_line <- paste(paste(indivs[i], suff, sep = ""), reg, sep = "\t")
    }
  
    if(i == 1){
      cat(tmp_line, file = file_nm)
    } else {
      cat(tmp_line, file = file_nm, append = TRUE)
    }
  
  })
 
}))

# Opened these files in Vim to doublecheck structure, looks good

```

The whitelist of the 320 neutral loci made in "FST_calculations.Rmd" will be used to retain these neutral loci for each VCF file. 

Once these were made, I uploaded to the info folder on Discovery, and made a new script to run Stacks::populations with the whitelist of 320 neutral loci to get VCF files so as to calculate the observed mSFS with easySFS.py.

# Finding optimla bins

From the delimitR manual: "The user must specify how many bins will be used to summarize the SFS. This number should not be greater than the sample size of the population with the fewest samples, as this results in sparse sampling of the SFS. Large values lead to a more complete summary of the data, but also lead to a more sparsely sampled SFS and increased computation times. Users should use exploratory analyses to find the optimal trade-off between computational time and error rates. The user must specify the number of classes that will be used in the binned SFS."

From Smith et al. 2017: "To determine the optimal binning strategy, eight RF classifiers were constructed using the simulated data (i.e., Figure 3, Step 4), one at each level of mSFS coarseness considered here (i.e., 3–10 classes per population). Each classifier was constructed with 500 trees using the R package “ABCRF” (Pudlo et al., 2015), with the bins of the mSFS trea- ted as the predictor variables and the generating model for each simu- lated data set treated as the response variable. At each node in each decision tree, the RF classifier considers a bin of the mSFS and con- structs a binary decision rule based on the number of SNPs in the bin."

Pretty sure this means constructing different priors with the makeprior() function, and then assessing the OOB error over the number of bins used.

Since I'm testing each invasive population separately, with 6 models each, the optimal number of bins should be assessed with each invasive population. 

Here, testing with BARC and CNCT.

Create a traits file.
```{r echo = TRUE, eval = FALSE}

tmp_path <- file.path(out_path, "ABC_BARC")

# Make a traits file
file_nm <- "popmap_mergedSNPs_ABCmodelling_BARC.txt"

pop_map <- read.table(file.path(map_path, file_nm), sep = "\t")
str(pop_map)
# nrow(pop_map)

# Substitute populations with a zero-based naming system
pop_map$V2 <- as.numeric(gsub("pop", "", pop_map$V2))
pop_map$V2 <- pop_map$V2
  
# Remove previous versions
file.remove(file.path(tmp_path, "traits.txt"))

# Print the header
header <- c(paste(c("traits", "species"), collapse = "\t"), "\n", sep = "")
cat(header, file = file.path(tmp_path, "traits.txt"), append = TRUE, sep = "")

# Print rows
  
# x <- 1
invisible(pblapply(1:nrow(pop_map), function(x){
  
  # Initialize file name
  tmp_nm <- file.path(tmp_path, "traits.txt")

  # Add two lines per individual, each represents an allele
  if(x != nrow(pop_map)){
    cat(paste(paste(pop_map[x, 1], "allele_1", sep = "_"), pop_map[x, 2], collapse = "\t"), "\n", file = tmp_nm, sep = "", append = TRUE)
    cat(paste(paste(pop_map[x, 1], "allele_2", sep = "_"), pop_map[x, 2], collapse = "\t"), "\n", file = tmp_nm, sep = "", append = TRUE)
  # Very last individual should not have a new line added after the second allele
  } else {
    cat(paste(paste(pop_map[x, 1], "allele_1", sep = "_"), pop_map[x, 2], collapse = "\t"), "\n", file = tmp_nm, sep = "", append = TRUE)
    cat(paste(paste(pop_map[x, 1], "allele_2", sep = "_"), pop_map[x, 2], collapse = "\t"), file = tmp_nm, sep = "", append = TRUE)
  }

}))

# Looks good

```

Build RF classifiers with different numbers of bins. 5 individuals (10 samples because diploid). The process will look like this: iterate over numbers of classes, build a full prior, reduce the prior, upload to Discovery and train all priors, then download and plot average OOB error over the number of bins. Until delimitR is installed on Discovery, I have to do most of this (except model training and plotting) on my local machine.

Do this for FLOR as well, to test a smaller and larger invasive population, respectively....actually, might be able to just skip this step by just using one less than the smallest sample sizes per population, or an intermediate sample size, to minimize computational time. If delimitR is installed on Discovery soon, then computational time is much less of an issue. The smallest sample sizes used for any invasive population or model was 8 (4 diploid individuals for each WASH and ZARA). The largest was 28 (14 individuals each for URY, NAR, FLOR, CNCT).

How long does it take to generate 9 classes for BARC?
```{r echo = TRUE, eval = FALSE}

tmp_path <- file.path(out_path, "ABC_BARC")

nClasses <- 9

obsprefix <- "BARC_model"
obsspecies <- 3 # Number of demes
traitsfile <- "traits.txt"
cores <- parallel::detectCores() - 2

# This function assumes the working directory has been set to the folder containing fcs26 output folders
# Must remove any folder previously made for priors (called "Prior*")
setwd(tmp_path)

system.time(
    
  FullPrior <- makeprior(
    prefix = obsprefix,
    nspec = obsspecies,
    nclasses = nClasses,
    mydir = tmp_path,
    traitsfile = traitsfile,
    threshold = 100,
    thefolder = "Prior",
    ncores = cores
  )
    # str(FullPrior)
    
)


# took about 30 minutes
#    user   system  elapsed 
# 9112.966  333.551 2163.775 

saveRDS(FullPrior, file.path(tmp_path, "FullPrior_9bins.RDS"))

# Remove rows with zero variance, e.g. bins for which no SNPs were ever observed across all models and simulations. 
ReducedPrior <- Prior_reduced(FullPrior)
# str(ReducedPrior)

saveRDS(ReducedPrior, file.path(tmp_path, paste("ReducedPrior_9bins.RDS", sep = "")))
    
rm(list = c("FullPrior", "ReducedPrior"))

```

How long does it take to generate 27 classes for CNCT?
Create a traits file.
```{r echo = TRUE, eval = FALSE}

tmp_path <- file.path(out_path, "ABC_CNCT")

# Make a traits file
file_nm <- "popmap_mergedSNPs_ABCmodelling_CNCT.txt"

pop_map <- read.table(file.path(map_path, file_nm), sep = "\t")
str(pop_map)
# nrow(pop_map)

# Substitute populations with a zero-based naming system
pop_map$V2 <- as.numeric(gsub("pop", "", pop_map$V2))
pop_map$V2 <- pop_map$V2
  
# Remove previous versions
file.remove(file.path(tmp_path, "traits.txt"))

# Print the header
header <- c(paste(c("traits", "species"), collapse = "\t"), "\n", sep = "")
cat(header, file = file.path(tmp_path, "traits.txt"), append = TRUE, sep = "")

# Print rows
  
# x <- 1
invisible(pblapply(1:nrow(pop_map), function(x){
  
  # Initialize file name
  tmp_nm <- file.path(tmp_path, "traits.txt")

  # Add two lines per individual, each represents an allele
  if(x != nrow(pop_map)){
    cat(paste(paste(pop_map[x, 1], "allele_1", sep = "_"), pop_map[x, 2], collapse = "\t"), "\n", file = tmp_nm, sep = "", append = TRUE)
    cat(paste(paste(pop_map[x, 1], "allele_2", sep = "_"), pop_map[x, 2], collapse = "\t"), "\n", file = tmp_nm, sep = "", append = TRUE)
  # Very last individual should not have a new line added after the second allele
  } else {
    cat(paste(paste(pop_map[x, 1], "allele_1", sep = "_"), pop_map[x, 2], collapse = "\t"), "\n", file = tmp_nm, sep = "", append = TRUE)
    cat(paste(paste(pop_map[x, 1], "allele_2", sep = "_"), pop_map[x, 2], collapse = "\t"), file = tmp_nm, sep = "", append = TRUE)
  }

}))

# Looks good

```

```{r echo = TRUE, eval = FALSE}

tmp_path <- file.path(out_path, "ABC_CNCT")

nClasses <- 27

obsprefix <- "CNCT_model"
obsspecies <- 3 # Number of demes
traitsfile <- "traits.txt"
cores <- parallel::detectCores() - 2

# This function assumes the working directory has been set to the folder containing fcs26 output folders
# Must remove any folder previously made for priors (called "Prior*")
setwd(tmp_path)

system.time(
    
  FullPrior <- makeprior(
    prefix = obsprefix,
    nspec = obsspecies,
    nclasses = nClasses,
    mydir = tmp_path,
    traitsfile = traitsfile,
    threshold = 100,
    thefolder = "Prior",
    ncores = cores
  )
    # str(FullPrior)
    
)


# took about TKTK minutes

saveRDS(FullPrior, file.path(tmp_path, "FullPrior_27bins.RDS"))

# Remove rows with zero variance, e.g. bins for which no SNPs were ever observed across all models and simulations. 
ReducedPrior <- Prior_reduced(FullPrior)
# str(ReducedPrior)

saveRDS(ReducedPrior, file.path(tmp_path, paste("ReducedPrior_27bins.RDS", sep = "")))
    
rm(list = c("FullPrior", "ReducedPrior"))

```


## Testing delimitR model selection with fsc26 simulated data

Using 4 bins because this was the minimum number of individuals used for any sampling site. Testing with BARC. From delimitRmanual: "If you conducted downsampling using scripts provided in the BuildmSFS directory (or another method), the traits file provided will be pre-downsampling and consist of the original individuals used to construct the SFS."
```{r echo = TRUE, eval = TRUE}

tmp_path <- file.path(out_path, "ABC_BARC")

# Make a traits file
file_nm <- "popmap_mergedSNPs_ABCmodelling_BARC.txt"

pop_map <- read.table(file.path(tmp_path, file_nm), sep = "\t")
str(pop_map)
# nrow(pop_map)

# Substitute populations with a zero-based naming system
pop_map$V2 <- as.numeric(gsub("pop", "", pop_map$V2))
pop_map$V2 <- pop_map$V2 - 1
  
# Remove previous versions
file.remove(file.path(tmp_path, "traits.txt"))

# Print the header
header <- c(paste(c("traits", "species"), collapse = "\t"), "\n", sep = "")
cat(header, file = file.path(tmp_path, "traits.txt"), append = TRUE, sep = "")

# Print rows
  
# x <- 1
invisible(pblapply(1:nrow(pop_map), function(x){
  
  # Initialize file name
  tmp_nm <- file.path(tmp_path, "traits.txt")

  # Add two lines per individual, each represents an allele
  if(x != nrow(pop_map)){
    cat(paste(paste(pop_map[x, 1], "allele_1", sep = "_"), pop_map[x, 2], collapse = "\t"), "\n", file = tmp_nm, sep = "", append = TRUE)
    cat(paste(paste(pop_map[x, 1], "allele_2", sep = "_"), pop_map[x, 2], collapse = "\t"), "\n", file = tmp_nm, sep = "", append = TRUE)
  # Very last individual should not have a new line added after the second allele
  } else {
    cat(paste(paste(pop_map[x, 1], "allele_1", sep = "_"), pop_map[x, 2], collapse = "\t"), "\n", file = tmp_nm, sep = "", append = TRUE)
    cat(paste(paste(pop_map[x, 1], "allele_2", sep = "_"), pop_map[x, 2], collapse = "\t"), file = tmp_nm, sep = "", append = TRUE)
  }

}))

# Looks good

nClasses <- 4

obsprefix <- "BARC_model"
obsspecies <- 3 # Number of demes
traitsfile <- "traits.txt"
cores <- parallel::detectCores() - 2

# This function assumes the working directory has been set to the folder containing fcs26 output folders
# Must remove any folder previously made for the prior (called "Prior")
setwd(tmp_path)
FullPrior <- makeprior(
  prefix = obsprefix,
  nspec = obsspecies,
  nclasses = nClasses,
  mydir = tmp_path,
  traitsfile = traitsfile,
  threshold = 100,
  thefolder = "Prior",
  ncores = cores
)
str(FullPrior)

saveRDS(FullPrior, file.path(tmp_path, "FullPrior.RDS"))

# Remove rows with zero variance, e.g. bins for which no SNPs were ever observed across all models and simulations. 
FullPrior <- readRDS(file.path(tmp_path, "FullPrior.RDS"))

ReducedPrior <- Prior_reduced(FullPrior)
str(ReducedPrior)

saveRDS(ReducedPrior, file.path(tmp_path, "ReducedPrior.RDS"))

# Build a random forests classifier that uses the bSFS (binned SFS) bins as predictors, and the model used for data simulation is the response. the abcrf package is used here. The number of trees can be supplied by the user. 

# Each decision tree in the forest uses a subset of the prior (still not sure what this means), and each node considers a bin of the bSFS (does this mean mtry is set to 1?). Then a binary decision rule is made based on the number of SNPs per bin. Observed data can be used as a prediction dataset that is run down the trained RF classifier, and the each decision tree votes for a demographic model. Since this is an RF classifier, the model with the greatest number of votes (leaves in the forest) wins.

# Error: cannot allocate vector of size 1.4 Mb
# In addition: Warning message:
# In lda.default(x, grouping, ...) : variables are collinear
# barc_RF <- RF_build_abcrf(ReducedPrior, FullPrior, 500)
# barc_RF

# Code from RF_build_abcrf
Models <- as.factor(FullPrior[, "Model"])

# rm(list = "FullPrior")

# Trainingdata <- data.frame(Models, ReducedPrior)
# RF <- abcrf::abcrf(Models ~ ., data = Trainingdata, ntree = 500, 
        # paral = TRUE)

# I did training on Discovery
barc_RF <- readRDS(file.path(tmp_path, "trained_reduced_Prior.RDS"))
str(barc_RF)
class(barc_RF)

ReducedPrior <- readRDS(file.path(tmp_path, "ReducedPrior.RDS"))
str(ReducedPrior)

# 60k simulations...looks good
# Low OOB error rates but also not all 0%, also looks good
barc_RF

# A plot of the reference table on LDA axes, in other words, separation of te simulated data by model in lower dimensional space, the reference table is used in model choice
plot(barc_RF, training = ReducedPrior)

# Run the observed BARC mSFS down the trained model
# Get the observed data into the correct format
observedSFS <- file.path(tmp_path, "BARC_MSFS")
traits <- file.path(tmp_path, traitsfile)

# From earlier in delimitR tutorial: The user must also provide a vector of sample sizes. Again, this must be compatible with the traits file and the observed SFS. The sample sizes should be specified in order from population 0 to population n-1. These should be the numbers AFTER downsampling....earlly? wasn't it before downsampling

# How did downsampling of the mSFS change sample sizes? I may have made the prior with the wrong threshold
# The observed mSFS has 3 demes, then 36, 10, 24, corresponding to 18, 5, and 12 samples
# But I wan BARC fsc26 simulations with original sample sizes, which should be fine
# Also keep in mind that prior to training, really should make classifiers to hoptimhize bins

# (36 + 10 + 24)/(40 + 10 + 28) # doesn't work as threshold

barc_observed <- prepobserved(
  observed = observedSFS,
  FullPrior = FullPrior,
  ReducedPrior = ReducedPrior,
  nclasses = nClasses,
  npops = obsspecies,
  traitsfile = traits,
  threshold = 100
)

# threshold: The percentage of individuals used to build the mSFS. This is relevant if you used a downsampling approach to build the mSFS from the observed data, but used the original number of individuals for generating models in delimitR.

# 09 October 2020: Found this in the file /home/owner/R/i686-pc-linux-gnu-library/3.6/delimitR/SFS_CreateBinned_3pops.py

# if len(values) != len(self):
        # FIXME: correct error to raise?
        # raise ValueError('Value list is not the same length as the '
            # 'OrderedDict.')
    # self.update(zip(self, values))

# Full error:
# Traceback (most recent call last):
#   File "/home/owner/R/i686-pc-linux-gnu-library/3.6/delimitR/SFS_CreateBinned_3pops.py", line 334, in <module>
#     Populate_AFS(file)
#   File "/home/owner/R/i686-pc-linux-gnu-library/3.6/delimitR/SFS_CreateBinned_3pops.py", line 326, in Populate_AFS
#     setvalues(Full_AFS,line)
#   File "/home/owner/R/i686-pc-linux-gnu-library/3.6/delimitR/SFS_CreateBinned_3pops.py", line 31, in setvalues
#     raise ValueError('Value list is not the same length as the '
# ValueError: Value list is not the same length as the OrderedDict.
# Error in file(file, "rt") : cannot open the connection
# In addition: Warning message:
# In file(file, "rt") :
#   cannot open file '/media/owner/MYIOPSITTA/R/Origins_Selection/ABC/ABC_BARC/BARC_MSFS_binned.obs': No such file or directory

# Wow when I commented out the lines in the Python script, I no longer got that error

# Running BARC prediction 09 Oct 2020
barc_prediction <- RF_predict_abcrf(barc_RF, barc_observed, ReducedPrior, FullPrior, 500)
barc_prediction

     # selected model votes model1 votes model2 votes model3 votes model4
# 1 BARC_model_1_MSFS          116           95           88           89
  # votes model5 votes model6 post.proba
# 1           61           51  0.9792167

# Wow Model 1 selected: URY origin, shorter bottleneck!! 

summary(barc_prediction)

# Number of affectations per model:
# BARC_model_1_MSFS BARC_model_2_MSFS BARC_model_3_MSFS BARC_model_4_MSFS 
                # 1                 0                 0                 0 
# BARC_model_5_MSFS BARC_model_6_MSFS 
                # 0                 0 

# post.proba = ABC-RF approximations of the posterior probability of the selected model for each observed dataset.
# Note that Model 2 got the second number of highest votes

```


### OLDER CODE

Make a new data frame with metdata for these samples. Using DAPC output here. 
```{r echo = TRUE, eval = TRUE}

# Get sample locations from metadata in the same order of samples in the genind object
grp1 <- sapply(1:nrow(neutral_snps@tab), function(i){
  meta_dats$Site_Code[grep(paste("^", dimnames(neutral_snps@tab)[[1]][i], "$", sep = ""), meta_dats$Sample_Name)]
})
head(grp1)
length(grp1)

# Population identifiers by country, then sampling site
# Same order as in Structure file (taken from BayeScan_post-processing.Rmd)
all_sites <- c("CHAC", "ELGE", "EMBR", "SEMI-3", "SOLE", "1135", "1145", "BAGU", "BCAR", "PEIX", "ALGA", "BAIR", "LURO", "BAZO", "ERIO", "BARC", "MADR", "MALL", "SEVI", "ZARA", "GRCA", "FLOR", "ILLI", "WASH", "CNCT")

# Change levels to be in order of country and sampling sites above
grp1 <- factor(grp1, levels = all_sites)

# If all PCs used, then no points will show up, just labels for sampling sites
# Here selected the number of PCs that contained 60% of variation, and 2 discriminant functions
dapc1 <- dapc(neutral_snps, pop = grp1, n.da = 2, pca.select = "percVar", perc.pca = 60)

unique(meta_dats$Region)

all_regs <- c("Southwestern Uruguay", "South Central Uruguay", "Northern Argentina", "Southern Argentina", "Spain", "Southern U.S.", "Northern U.S.")

region <- meta_dats %>%
  filter(Sample_Name %in% dimnames(dapc1$ind.coord)[[1]]) %>%
  pull(Region)

dapc_df <- data.frame(
  X = dapc1$ind.coord[, 1],
  Y = dapc1$ind.coord[, 2]
  ) %>%
  dplyr::mutate(
    type = rep("Neutral merged SNPs", nrow(dapc1$ind.coord)),
    indiv = dimnames(dapc1$ind.coord)[[1]],
    region = gsub("United States", "U.S.", region),
    region = factor(region, levels = all_regs),
    site = grp1
  )

glimpse(dapc_df)

```

Make new files called "ABC_populations_modelX.txt" that contain strata or populations for each ABC model (necessary for writing out VCF file and getting observed SFS per ABC model).

Initialize all unique demes to be used across models: URY-NAR, URY, NAR, SAR, SPA, USA, CNCT. 
```{r echo = TRUE, eval = TRUE}

# URY-NAR, 59 individuals
ury_nar <- dapc_df %>%
  filter(grepl("Uruguay|Northern Argentina", region)) %>%
  pull(indiv) 

head(ury_nar)
length(ury_nar)

# URY, 45 individuals
ury <- dapc_df %>%
  filter(grepl("Uruguay", region)) %>%
  pull(indiv) 

head(ury)
length(ury)
  
# NAR, 14 individuals
nar <- dapc_df %>%
  filter(grepl("Northern Argentina", region)) %>%
  pull(indiv) 

head(nar)
length(nar)  

# SAR, 18 individuals
sar <- dapc_df %>%
  filter(grepl("Southern Argentina", region)) %>%
  pull(indiv) 

head(sar)
length(sar)

```

# Native range ABC modelling

Models 1 and 3: 2 demes. Make a tab delimited file with column headers INDIVIDUALS and STRATA for radiator::genomic_converter. Make a tab delimited file without headers for the easy_SFS.py script.
```{r echo = TRUE, eval = FALSE}

# Make a temporary data frame for Model 1
tmp_df <- dapc_df %>%
  filter(indiv %in% c(ury, nar, sar)) %>%
  droplevels() %>%
  dplyr::mutate(
    region = as.character(region),
    region = recode(
      region,
      `Southwestern Uruguay` = "URY-NAR",
      `South Central Uruguay` = "URY-NAR",
      `Northern Argentina` = "URY-NAR",
      `Southern Argentina` = "SAR"
    )
  ) %>%
  dplyr::mutate(
    pop = region,
    pop = recode(
      pop,
      `URY-NAR` = "pop1",
      `SAR` = "pop2"
    )
  )
glimpse(tmp_df)  
# View(tmp_df) # looks good

# Order by population
tmp_df <- tmp_df %>%
  arrange(-desc(pop))

glimpse(tmp_df) 

# Text file for radiator
file_nm <- "radiator_strata_models_1and3.tsv"
file.remove(file.path(out_path, file_nm))

header <- c(paste(c("INDIVIDUALS", "STRATA"), collapse = "\t"), "\n")

cat(header, file = file.path(out_path, file_nm), append = TRUE, sep = "")

invisible(pbsapply(1:nrow(tmp_df), function(i){
  
  # Substituted "_" for "-" as this was done in radiator for sample names when convertin to VCF
  cat(c(gsub("_", "-", tmp_df$indiv[i]), "\t", tmp_df$pop[i], "\n"), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))


# Text file for easy_SFS.py (no column headers)
file_nm <- "easySFS_popfile_models_1and3.txt"
file.remove(file.path(out_path, file_nm))

invisible(pbsapply(1:nrow(tmp_df), function(i){
  
  # Substituted "_" for "-" as this was done in radiator for sample names when convertin to VCF
  cat(c(gsub("_", "-", tmp_df$indiv[i]), "\t", tmp_df$pop[i], "\n"), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))

# Opened both files in Vim, both look good

```

Models 2 and 4: 3 demes.
```{r echo = TRUE, eval = FALSE}

# Make a temporary data frame for Model 2
tmp_df <- dapc_df %>%
  filter(indiv %in% c(ury, nar, sar)) %>%
  droplevels() %>%
  dplyr::mutate(
    region = as.character(region),
    region = recode(
      region,
      `Southwestern Uruguay` = "URY",
      `South Central Uruguay` = "URY",
      `Northern Argentina` = "NAR",
      `Southern Argentina` = "SAR"
    )
  ) %>%
  dplyr::mutate(
    pop = region,
    pop = recode(
      pop,
      `URY` = "pop1",
      `NAR` = "pop2",
      `SAR` = "pop3"
    )
  )
glimpse(tmp_df)  
# View(tmp_df) # looks good

# Order by population
tmp_df <- tmp_df %>%
  arrange(-desc(pop))

glimpse(tmp_df)

# Text file for radiator
file_nm <- "radiator_strata_models_2and4.tsv"
file.remove(file.path(out_path, file_nm))

header <- c(paste(c("INDIVIDUALS", "STRATA"), collapse = "\t"), "\n")

cat(header, file = file.path(out_path, file_nm), append = TRUE, sep = "")

invisible(pbsapply(1:nrow(tmp_df), function(i){
  
  # Substituted "_" for "-" as this was done in radiator for sample names when convertin to VCF
  cat(c(gsub("_", "-", tmp_df$indiv[i]), "\t", tmp_df$pop[i], "\n"), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))


# Text file for easy_SFS.py (no column headers)
file_nm <- "easySFS_popfile_models_2and4.txt"
file.remove(file.path(out_path, file_nm))

invisible(pbsapply(1:nrow(tmp_df), function(i){
  
  # Substituted "_" for "-" as this was done in radiator for sample names when convertin to VCF
  cat(c(gsub("_", "-", tmp_df$indiv[i]), "\t", tmp_df$pop[i], "\n"), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))

# Opened both files in Vim, both look good

```

Used radiator to convert from genind format to VCF per model, write out VCF files per models with the same demes. radiator path setting is bad: assumes a path inside thre current working directory, so I have to set my directory for it to work
```{r echo = TRUE, eval = FALSE}

setwd(out_path)

sfiles <- list.files(out_path, pattern = "^radiator_", full.names = TRUE)
sfiles

runs <- seq(1, 2, 1)
suffs <- c("models_1and3", "models_2and4")

class(neutral_snps)
detect_genomic_format(neutral_snps)

# i <- 1
invisible(pblapply(1:length(runs), function(i){
  genomic_converter(data = neutral_snps, output = "vcf", filename = paste("ABC_", suffs[i], sep = ""), strata = sfiles[i], vcf.metadata = FALSE, vcf.stats = FALSE)
  # any(is.na(tmp$tidy.data$MARKERS))
  # any(is.na(tmp$tidy.data$POP_ID))
  # any(is.na(tmp$tidy.data$INDIVIDUALS))
  # any(is.na(tmp$tidy.data$GT))
  # any(is.na(tmp$tidy.data$REF))
  # any(is.na(tmp$tidy.data$ALT))
  # any(is.na(tmp$tidy.data$GT_VCF))
  # any(is.na(tmp$tidy.data$GT_BIN)) # This column has NAs....
  
  # Works just fine, underscores are automatically changed to dashes
  # read_strata(sfiles[i])
  
}))

# For every run I get an error that a separator is not valid, despite changing all "_" to "-" but the strata file is read in just fine and the package detects the genind class

# I also get an error about NAs created which I think is related to the NA in the POS field in the resulting VCF file, but everything else looks good:
# Warning messages:
# 1: Problem with `mutate()` input `LOCUS`.
# ℹ NAs introduced by coercion
# ℹ Input `LOCUS` is `stringi::stri_join(LOCUS, as.numeric(POS) - 1, sep = "_")`. 
# 2: In stringi::stri_join(LOCUS, as.numeric(POS) - 1, sep = "_") :
#   NAs introduced by coercion

# Looks like this just an error from reading in data with tifyr?
# Resulting VCF file looks different compared to Stacks VCF file, has just genotypes

```

I moved the resulting VCF files out of the ./radiator_genomic_converter* directories and into /media/owner/MYIOPSITTA/R/Origins_Selection/ABC/.

Followed instructions for calculating observed SFSs in this GitHub repo: https://github.com/isaacovercast/easySFS. I used Pycharm to use a virtual environment with Python3.

After using easySFS.py to downproject and get mSFS, I started running simulations for these 4 models on Discovery with fastsimcoal2.6. See the delimitR tutorial for more details.

 



########## OLDER, Update as needed

Initialize all unique demes to be used across models: URY-NAR, URY, NAR, SAR, SPA, USA, CNCT. 
```{r echo = TRUE, eval = TRUE}

# URY-NAR, 59 individuals
ury_nar <- dapc_df %>%
  filter(grepl("Uruguay|Northern Argentina", region)) %>%
  pull(indiv) 

head(ury_nar)
length(ury_nar)

# URY, 45 individuals
ury <- dapc_df %>%
  filter(grepl("Uruguay", region)) %>%
  pull(indiv) 

head(ury)
length(ury)
  
# NAR, 14 individuals
nar <- dapc_df %>%
  filter(grepl("Northern Argentina", region)) %>%
  pull(indiv) 

head(nar)
length(nar)  

# SAR, 18 individuals
sar <- dapc_df %>%
  filter(grepl("Southern Argentina", region)) %>%
  pull(indiv) 

head(sar)
length(sar)

# SPA, 28 individuals
spa <- dapc_df %>%
  filter(grepl("Spain", region)) %>%
  pull(indiv) 

head(spa)
length(spa)  

# USA (no CNCT), 41 individuals
usa <- dapc_df %>%
  filter(grepl("U.S.", region) & !grepl("CNCT", site)) %>%
  pull(indiv) 

head(usa)
length(usa)

# CNCT, 27 individuals
cnct <- dapc_df %>%
  filter(grepl("CNCT", site)) %>%
  pull(indiv) 

head(cnct)
length(cnct)

```

Model 1: 5 demes. Make a tab delimited file with column headers INDIVIDUALS and STRATA for radiator::genomic_converter. Make a tab delimited file without headers for the easy_SFS.py script.
```{r echo = TRUE, eval = FALSE}

# Make a temporary data frame for Model 1
tmp_df <- dapc_df %>%
  filter(indiv %in% c(ury_nar, spa, cnct, usa, sar)) %>%
  droplevels() %>%
  dplyr::mutate(
    region = as.character(region),
    region = recode(
      region,
      `Southwestern Uruguay` = "URY-NAR",
      `South Central Uruguay` = "URY-NAR",
      `Northern Argentina` = "URY-NAR",
      `Spain` = "SPA",
      `Northern U.S.` = "USA",
      `Southern U.S.` = "USA",
      `Southern Argentina` = "SAR"
    ),
    region = ifelse(site == "CNCT", "CNCT", region)
  ) %>%
  dplyr::mutate(
    pop = region,
    pop = recode(
      pop,
      `URY-NAR` = "pop1",
      `SPA` = "pop2",
      `CNCT` = "pop3",
      `USA` = "pop4",
      `SAR` = "pop5"
    )
  )
glimpse(tmp_df)  
# View(tmp_df) # looks good

# Order by population
tmp_df <- tmp_df %>%
  arrange(-desc(pop))

glimpse(tmp_df) 

# Text file for radiator
file_nm <- "radiator_strata_model_1.tsv"
file.remove(file.path(out_path, file_nm))

header <- c(paste(c("INDIVIDUALS", "STRATA"), collapse = "\t"), "\n")

cat(header, file = file.path(out_path, file_nm), append = TRUE, sep = "")

invisible(pbsapply(1:nrow(tmp_df), function(i){
  
  # Substituted "_" for "-" as this was done in radiator for sample names when convertin to VCF
  cat(c(gsub("_", "-", tmp_df$indiv[i]), "\t", tmp_df$pop[i], "\n"), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))


# Text file for easy_SFS.py (no column headers)
file_nm <- "easySFS_popfile_model_1.txt"
file.remove(file.path(out_path, file_nm))

invisible(pbsapply(1:nrow(tmp_df), function(i){
  
  # Substituted "_" for "-" as this was done in radiator for sample names when convertin to VCF
  cat(c(gsub("_", "-", tmp_df$indiv[i]), "\t", tmp_df$pop[i], "\n"), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))

# Opened both file in Vim, both look good

```

Model 2: 6 demes.
```{r echo = TRUE, eval = FALSE}

# Make a temporary data frame for Model 2
tmp_df <- dapc_df %>%
  filter(indiv %in% c(ury_nar, spa, cnct, usa, sar)) %>%
  droplevels() %>%
  dplyr::mutate(
    region = as.character(region),
    region = recode(
      region,
      `Southwestern Uruguay` = "URY",
      `South Central Uruguay` = "URY",
      `Northern Argentina` = "NAR",
      `Spain` = "SPA",
      `Northern U.S.` = "USA",
      `Southern U.S.` = "USA",
      `Southern Argentina` = "SAR"
    ),
    region = ifelse(site == "CNCT", "CNCT", region)
  ) %>%
  dplyr::mutate(
    pop = region,
    pop = recode(
      pop,
      `URY` = "pop1",
      `SPA` = "pop2",
      `CNCT` = "pop3",
      `USA` = "pop4",
      `NAR` = "pop5",
      `SAR` = "pop6"
    )
  )
glimpse(tmp_df)  
# View(tmp_df) # looks good

# Order by population
tmp_df <- tmp_df %>%
  arrange(-desc(pop))

glimpse(tmp_df)

# Text file for radiator
file_nm <- "radiator_strata_model_2.tsv"
file.remove(file.path(out_path, file_nm))

header <- c(paste(c("INDIVIDUALS", "STRATA"), collapse = "\t"), "\n")

cat(header, file = file.path(out_path, file_nm), append = TRUE, sep = "")

invisible(pbsapply(1:nrow(tmp_df), function(i){
  
  # Substituted "_" for "-" as this was done in radiator for sample names when convertin to VCF
  cat(c(gsub("_", "-", tmp_df$indiv[i]), "\t", tmp_df$pop[i], "\n"), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))


# Text file for easy_SFS.py (no column headers)
file_nm <- "easySFS_popfile_model_2.txt"
file.remove(file.path(out_path, file_nm))

invisible(pbsapply(1:nrow(tmp_df), function(i){
  
  # Substituted "_" for "-" as this was done in radiator for sample names when convertin to VCF
  cat(c(gsub("_", "-", tmp_df$indiv[i]), "\t", tmp_df$pop[i], "\n"), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))

# Opened both file in Vim, both look good

```

For models 3 and 4, I just copied the text files made for model 2, as these had the same 6 demes. 

Use radiator to convert from genind format to VCF per model, write out VCF files per model. VCF files are the same for models 2 - 4. radiator path setting is bad: assumes a path inside thre current working directory, so I have to set my directory for it to work
```{r echo = TRUE, eval = FALSE}

setwd(out_path)

sfiles <- list.files(out_path, pattern = "^radiator_", full.names = TRUE)
sfiles

models <- seq(1, 4, 1)

class(neutral_snps)
detect_genomic_format(neutral_snps)

# i <- 1
invisible(pblapply(1:length(models), function(i){
  genomic_converter(data = neutral_snps, output = "vcf", filename = paste("ABC_model_", i, sep = ""), strata = sfiles[i], vcf.metadata = FALSE, vcf.stats = FALSE)
  # any(is.na(tmp$tidy.data$MARKERS))
  # any(is.na(tmp$tidy.data$POP_ID))
  # any(is.na(tmp$tidy.data$INDIVIDUALS))
  # any(is.na(tmp$tidy.data$GT))
  # any(is.na(tmp$tidy.data$REF))
  # any(is.na(tmp$tidy.data$ALT))
  # any(is.na(tmp$tidy.data$GT_VCF))
  # any(is.na(tmp$tidy.data$GT_BIN)) # This column has NAs....
  
  # Works just fine, underscores are automatically changed to dashes
  # read_strata(sfiles[i])
  
}))

# For every run I get an error that a separator is not valid, despite changing all "_" to "-" but the strata file is read in just fine and the package detects the genind class

# I also get an error about NAs created which I think is related to the NA in the POS field in the resulting VCF file, but everything else looks good:
# Warning messages:
# 1: Problem with `mutate()` input `LOCUS`.
# ℹ NAs introduced by coercion
# ℹ Input `LOCUS` is `stringi::stri_join(LOCUS, as.numeric(POS) - 1, sep = "_")`. 
# 2: In stringi::stri_join(LOCUS, as.numeric(POS) - 1, sep = "_") :
#   NAs introduced by coercion

# Looks like this just an error from reading in data with tifyr?
# Resulting VCF file looks different compared to Stacks VCF file, has just genotypes

```

I moved the resulting VCF files out of the ./radiator_genomic_converter* directories and into /media/owner/MYIOPSITTA/R/Origins_Selection/ABC/.

Followed instructions for calculating observed SFSs in this GitHub repo: https://github.com/isaacovercast/easySFS. I used Pycharm to use a virtual environment with Python3.


# Downsampling individuals for ABC

30 September 2020: Making the multidimensional SFS with all individuals per population in each model fails, probably due to the relatively large number of individuals used. Now trying downsampling of individuals per population to be used for ABC.

Read in the dataset of pre-processed neutral SNPs from the merged dataset. The number of individuals and loci is documented in BayeScan_post-processing.Rmd.
```{r echo = TRUE, eval = TRUE}

file_nm <- "merged_HWE_missingData_filters_noPosControlDups_neutralSNPs.str"

neutral_snps <- read.structure(file.path(file.path(res_path, "merged"), file_nm), n.ind = 173, n.loc = 320, col.lab = 1, col.pop = 2, row.marknames = 1, onerowperind = FALSE, ask = FALSE, NA.char = "-9")
str(neutral_snps)

str(neutral_snps@tab)

```

Make a new data frame with metdata for these samples. Using DAPC output here. 
```{r echo = TRUE, eval = TRUE}

# Get sample locations from metadata in the same order of samples in the genind object
grp1 <- sapply(1:nrow(neutral_snps@tab), function(i){
  meta_dats$Site_Code[grep(paste("^", dimnames(neutral_snps@tab)[[1]][i], "$", sep = ""), meta_dats$Sample_Name)]
})
head(grp1)
length(grp1)

# Population identifiers by country, then sampling site
# Same order as in Structure file (taken from BayeScan_post-processing.Rmd)
all_sites <- c("CHAC", "ELGE", "EMBR", "SEMI-3", "SOLE", "1135", "1145", "BAGU", "BCAR", "PEIX", "ALGA", "BAIR", "LURO", "BAZO", "ERIO", "BARC", "MADR", "MALL", "SEVI", "ZARA", "GRCA", "FLOR", "ILLI", "WASH", "CNCT")

# Change levels to be in order of country and sampling sites above
grp1 <- factor(grp1, levels = all_sites)

# If all PCs used, then no points will show up, just labels for sampling sites
# Here selected the number of PCs that contained 60% of variation, and 2 discriminant functions
dapc1 <- dapc(neutral_snps, pop = grp1, n.da = 2, pca.select = "percVar", perc.pca = 60)

unique(meta_dats$Region)

all_regs <- c("Southwestern Uruguay", "South Central Uruguay", "Northern Argentina", "Southern Argentina", "Spain", "Southern U.S.", "Northern U.S.")

region <- meta_dats %>%
  filter(Sample_Name %in% dimnames(dapc1$ind.coord)[[1]]) %>%
  pull(Region)

dapc_df <- data.frame(
  X = dapc1$ind.coord[, 1],
  Y = dapc1$ind.coord[, 2]
  ) %>%
  dplyr::mutate(
    type = rep("Neutral merged SNPs", nrow(dapc1$ind.coord)),
    indiv = dimnames(dapc1$ind.coord)[[1]],
    region = gsub("United States", "U.S.", region),
    region = factor(region, levels = all_regs),
    site = grp1
  )

glimpse(dapc_df)

```

Make new files called "ABC_populations_modelX.txt" that contain strata or populations for each ABC model (necessary for writing out VCF file and getting observed SFS per ABC model).

Initialize all unique demes to be used across models: URY, NAR, SAR, SPA, USA, CNCT. 
```{r echo = TRUE, eval = TRUE}

# URY, 45 individuals
ury <- dapc_df %>%
  filter(grepl("Uruguay", region)) %>%
  pull(indiv) 

head(ury)
length(ury)
  
# NAR, 14 individuals
nar <- dapc_df %>%
  filter(grepl("Northern Argentina", region)) %>%
  pull(indiv) 

head(nar)
length(nar)  

# SAR, 18 individuals
sar <- dapc_df %>%
  filter(grepl("Southern Argentina", region)) %>%
  pull(indiv) 

head(sar)
length(sar)

# SPA, 28 individuals
spa <- dapc_df %>%
  filter(grepl("Spain", region)) %>%
  pull(indiv) 

head(spa)
length(spa)  

# USA (no CNCT), 41 individuals
usa <- dapc_df %>%
  filter(grepl("U.S.", region) & !grepl("CNCT", site)) %>%
  pull(indiv) 

head(usa)
length(usa)

# CNCT, 27 individuals
cnct <- dapc_df %>%
  filter(grepl("CNCT", site)) %>%
  pull(indiv) 

head(cnct)
length(cnct)

```

Read in the dataset of pre-processed neutral SNPs from the merged dataset as a .txt file, convert to data frame, subset by randomly sampled individuals per population, write out a new Structure file 
```{r echo = TRUE, eval = TRUE}

file_nm <- "merged_HWE_missingData_filters_noPosControlDups_neutralSNPs.str"

struc <- read.table(file.path(file.path(res_path, "merged"), file_nm), skip = 1)
# glimpse(struc)
dim(struc) # 346 rows (173 individuals), 322 columns
head(struc[, 1]) # Sample names contained in the first column
head(struc[, 2]) # Population ID (single population) contained in second column
names(struc)

# Get the column headers

# Read the first line of metadata, these are the column names
col_nms <- readLines(file.path(file.path(res_path, "merged"), file_nm), n = 1)
col_nms

# Split out the column headers into a vector, one element per header
# No space that precedes the first locus column header
mrkrs <- strsplit(strsplit(col_nms, split = "\t")[[1]][3], split = "[[:space:]]")[[1]]
head(mrkrs)
length(mrkrs)

# Add the individual and population column headers that precede the locus column names
col_nms <- c("indiv", "pop", mrkrs)
head(col_nms)
length(col_nms)

# Add back the updated column names
# Convert the indiv column to character
# Looks good
names(struc) <- col_nms
struc$indiv <- as.character(struc$indiv)
glimpse(struc[, 1:10])

```

Filter this Structure file by randomly sampling individuals per population to be used in ABC. Randomly sample 12 individuals each for URY, NAR, SAR, SPA, USA, CNCT, or take all indivs if less present. In model 1, the randomly sampled URY and NAR individuals will be combined into URY-NAR.
```{r echo = TRUE, eval = TRUE}

n <- 12

# For URY, randomly sample from two sites near exporter's aviaries in southcentral URY, and 2 sites in west 
set.seed(seed)
ury_r <- dapc_df %>%
  filter(grepl("Uruguay", region)) %>%
  filter(site %in% c("BCAR", "PEIX", "ELGE", "EMBR")) %>%
  pull(indiv) %>%
  sample(n, replace = FALSE)
ury_r

# Randomly sample 12 of 14 NAR
nar_r <- sample(nar, n, replace = FALSE)

# For Spain, randomly sample 12 birds across all 6 sites  
set.seed(seed)
spa_r <- dapc_df %>%
  filter(grepl("Spain", region)) %>%
  pull(indiv) %>%
  sample(n, replace = FALSE) 
spa_r

# For CNCT, randomly sample 12 birds
set.seed(seed)
cnct_r <- sample(cnct, n, replace = FALSE)
cnct_r

# For the U.S., use all 4 WASH, then randomly sample 4 from ILLI and 4 from FLOR
set.seed(seed)
usa_r <- dapc_df %>%
      filter(grepl("U.S.", region)) %>%
      filter(site == "WASH") %>%
      pull(indiv) %>%
  c(
    dapc_df %>%
      filter(grepl("U.S.", region)) %>%
      filter(site == "ILLI") %>%
      pull(indiv) %>%
      sample(4, replace = FALSE)
  ) %>%
  c(
    dapc_df %>%
      filter(grepl("U.S.", region)) %>%
      filter(site == "FLOR") %>%
      pull(indiv) %>%
      sample(4, replace = FALSE)
  )

usa_r

# 18 SAR individuals, randomly sample 12
set.seed(seed)
sar_r <- sample(sar, n, replace = FALSE)
sar_r

```

Subset the data frame by these individuals and write out a new Structure file to the ABC folder. Note that population IDs are the same as those used for Structure (sampling sites), but these will be updated in files made for ABC. 12 individuals per ABC population (not counting URY-NAR), 72 individuals total.
```{r echo = TRUE, eval = TRUE}

# 12 URY, 12 NAR, 12 SPA, 12 CNCT, 12 USA, 12 SAR
neutral_ds <- struc[grep(paste(paste("^", c(ury_r, nar_r, spa_r, cnct_r, usa_r, sar_r), "$", sep = ""), collapse = "|"), struc$indiv), ]
dim(neutral_ds) # 144 rows = 72 individuals, 320 SNPs
ncol(neutral_ds) - 2

file_nm <- "merged_HWE_missingData_filters_noPosControlDups_neutralSNPs_rsampIndivs.str"
file.remove(file.path(out_path, file_nm))

header <- c("\t","\t", paste(paste(names(neutral_ds)[-grep("^indiv$|^pop$", names(neutral_ds))]), collapse = " "), "\n", sep = "")

cat(header, file = file.path(out_path, file_nm), append = TRUE, sep = "")

invisible(pbsapply(1:nrow(neutral_ds), function(i){
  
  cat(c(neutral_ds$indiv[i], "\t", neutral_ds$pop[i], "\t", paste(paste(neutral_ds[i, -grep("^indiv$|^pop$", names(neutral_ds))], collapse = " "), "\n", sep = "")), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))

# Opened the file in Vim to doublecheck structure, looks good
# Use :set list in Vim to see invisible characters (tabs)

```

Read in the downsampled dataset and run DAPC as a visual check, also to get metadata.
```{r echo = TRUE, eval = TRUE}

file_nm <- "merged_HWE_missingData_filters_noPosControlDups_neutralSNPs_rsampIndivs.str"

neutral_snps_ds <- read.structure(file.path(out_path, file_nm), n.ind = 94, n.loc = 320, col.lab = 1, col.pop = 2, row.marknames = 1, onerowperind = FALSE, ask = FALSE, NA.char = "-9")
str(neutral_snps_ds)

str(neutral_snps_ds@tab)

```

Make a new data frame with metdata for these samples. Using DAPC output here. 
```{r echo = TRUE, eval = TRUE}

# Get sample locations from metadata in the same order of samples in the genind object
grp1 <- sapply(1:nrow(neutral_snps_ds@tab), function(i){
  meta_dats$Site_Code[grep(paste("^", dimnames(neutral_snps_ds@tab)[[1]][i], "$", sep = ""), meta_dats$Sample_Name)]
})
head(grp1)
length(grp1)
unique(grp1)

# CONTINUE, update colors, shapes

# Population identifiers by country, then sampling site
all_sites <- c("ELGE", "EMBR", "BCAR", "PEIX", "ALGA", "BAIR", "LURO", "BAZO", "ERIO", "BARC", "MADR", "ZARA", "GRCA", "MALL", "FLOR", "ILLI", "WASH", "CNCT")
length(all_sites)

# Change levels to be in order of country and sampling sites above
grp1 <- factor(grp1, levels = all_sites)

# If all PCs used, then no points will show up, just labels for sampling sites
# Here selected the number of PCs that contained 60% of variation, and 2 discriminant functions
dapc1 <- dapc(neutral_snps_ds, pop = grp1, n.da = 2, pca.select = "percVar", perc.pca = 30)

unique(meta_dats$Region)

all_regs <- c("Southwestern Uruguay", "South Central Uruguay", "Northern Argentina", "Southern Argentina", "Spain", "Southern U.S.", "Northern U.S.")

region <- meta_dats %>%
  filter(Sample_Name %in% dimnames(dapc1$ind.coord)[[1]]) %>%
  pull(Region)

dapc_df <- data.frame(
  X = dapc1$ind.coord[, 1],
  Y = dapc1$ind.coord[, 2]
  ) %>%
  dplyr::mutate(
    type = rep("Neutral merged SNPs", nrow(dapc1$ind.coord)),
    indiv = dimnames(dapc1$ind.coord)[[1]],
    region = gsub("United States", "U.S.", region),
    region = factor(region, levels = all_regs),
    site = grp1
  )

glimpse(dapc_df)
# View(dapc_df)

# Make colors by country, hues by sampling location
levels(grp1)

# Uruguay in blues, Argentina in grays, Spain in reds, US in golds
cols <- c("navy", "royalblue", "turquoise", "cornflowerblue", "black", "gray5", "gray25", "gray50", "gray75", "firebrick", "maroon", "brown", "coral", "orangered", "orange", "gold2", "yellow2", "gold4")
length(cols)

# Shapes by country
pchs <- c(rep(19, 4), rep(18, 5), rep(15, 5), rep(17, 4))
length(pchs)

dapc_df %>%
  ggplot(aes(x = X, y = Y, color = site, shape = site)) +
  geom_hline(aes(yintercept = 0), linetype = "solid", color = "black", size = 0.25) +
  geom_vline(aes(xintercept = 0), linetype = "solid", color = "black", size = 0.25) +
  geom_point(size = 6) +
  facet_wrap(~ type) +
  scale_shape_manual(values = pchs) +
  scale_color_manual(values = alpha(cols, 0.75)) +
  xlab("Dimension 1") +
  ylab("Dimension 2") +
  theme_bw()

# Patterns look similar to the full dataset of 173 individuals, except CNCT overlaps with all others and GRCA falls out as different when using perc.pca of 60. When using perc.pca of 30, patterns look more similar to full dataset of 173 individuals with perc.pca of 60

```

# Files for ABC with downsampled individuals

Model 1: 5 demes. Make a tab delimited file with column headers INDIVIDUALS and STRATA for radiator::genomic_converter. Make a tab delimited file without headers for the easy_SFS.py script.
```{r echo = TRUE, eval = FALSE}

ury_nar <- c(ury_r, nar_r)

# Make a temporary data frame for Model 1
tmp_df <- dapc_df %>%
  filter(indiv %in% c(ury_nar, spa_r, cnct_r, usa_r, sar)) %>%
  droplevels() %>%
  dplyr::mutate(
    region = as.character(region),
    region = recode(
      region,
      `Southwestern Uruguay` = "URY-NAR",
      `South Central Uruguay` = "URY-NAR",
      `Northern Argentina` = "URY-NAR",
      `Spain` = "SPA",
      `Northern U.S.` = "USA",
      `Southern U.S.` = "USA",
      `Southern Argentina` = "SAR"
    ),
    region = ifelse(site == "CNCT", "CNCT", region)
  ) %>%
  dplyr::mutate(
    pop = region,
    pop = recode(
      pop,
      `URY-NAR` = "pop1",
      `SPA` = "pop2",
      `CNCT` = "pop3",
      `USA` = "pop4",
      `SAR` = "pop5"
    )
  )
glimpse(tmp_df)  
# View(tmp_df) # looks good

# Order by population
tmp_df <- tmp_df %>%
  arrange(-desc(pop))

glimpse(tmp_df) 

# Text file for radiator
file_nm <- "radiator_strata_model_1.tsv"
file.remove(file.path(out_path, file_nm))

header <- c(paste(c("INDIVIDUALS", "STRATA"), collapse = "\t"), "\n")

cat(header, file = file.path(out_path, file_nm), append = TRUE, sep = "")

invisible(pbsapply(1:nrow(tmp_df), function(i){
  
  # Substituted "_" for "-" as this was done in radiator for sample names when convertin to VCF
  cat(c(gsub("_", "-", tmp_df$indiv[i]), "\t", tmp_df$pop[i], "\n"), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))


# Text file for easy_SFS.py (no column headers)
file_nm <- "easySFS_popfile_model_1.txt"
file.remove(file.path(out_path, file_nm))

invisible(pbsapply(1:nrow(tmp_df), function(i){
  
  # Substituted "_" for "-" as this was done in radiator for sample names when convertin to VCF
  cat(c(gsub("_", "-", tmp_df$indiv[i]), "\t", tmp_df$pop[i], "\n"), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))

# Opened both file in Vim, both look good

```

Models 2-4: 6 demes.
```{r echo = TRUE, eval = FALSE}

ury_nar <- c(ury_r, nar_r)

# Make a temporary data frame for Model 2
tmp_df <- dapc_df %>%
  filter(indiv %in% c(ury_nar, spa_r, cnct_r, usa_r, sar)) %>%
  droplevels() %>%
  dplyr::mutate(
    region = as.character(region),
    region = recode(
      region,
      `Southwestern Uruguay` = "URY",
      `South Central Uruguay` = "URY",
      `Northern Argentina` = "NAR",
      `Spain` = "SPA",
      `Northern U.S.` = "USA",
      `Southern U.S.` = "USA",
      `Southern Argentina` = "SAR"
    ),
    region = ifelse(site == "CNCT", "CNCT", region)
  ) %>%
  dplyr::mutate(
    pop = region,
    pop = recode(
      pop,
      `URY` = "pop1",
      `SPA` = "pop2",
      `CNCT` = "pop3",
      `USA` = "pop4",
      `NAR` = "pop5",
      `SAR` = "pop6"
    )
  )
glimpse(tmp_df)  
# View(tmp_df) # looks good

# Order by population
tmp_df <- tmp_df %>%
  arrange(-desc(pop))

glimpse(tmp_df)

# Text file for radiator
file_nm <- "radiator_strata_models_2to4.tsv"
file.remove(file.path(out_path, file_nm))

header <- c(paste(c("INDIVIDUALS", "STRATA"), collapse = "\t"), "\n")

cat(header, file = file.path(out_path, file_nm), append = TRUE, sep = "")

invisible(pbsapply(1:nrow(tmp_df), function(i){
  
  # Substituted "_" for "-" as this was done in radiator for sample names when convertin to VCF
  cat(c(gsub("_", "-", tmp_df$indiv[i]), "\t", tmp_df$pop[i], "\n"), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))


# Text file for easy_SFS.py (no column headers)
file_nm <- "easySFS_popfile_models_2to4.txt"
file.remove(file.path(out_path, file_nm))

invisible(pbsapply(1:nrow(tmp_df), function(i){
  
  # Substituted "_" for "-" as this was done in radiator for sample names when convertin to VCF
  cat(c(gsub("_", "-", tmp_df$indiv[i]), "\t", tmp_df$pop[i], "\n"), file = file.path(out_path, file_nm), append = TRUE, sep = "")
  
}))

# Opened both file in Vim, both look good

```

Note that populations are the same for models 3 and 4, so I didn't make separate files for these.

Use radiator to convert from genind format to VCF per model, write out VCF files per model. VCF files are the same for models 2 - 4. radiator path setting is bad: assumes a path inside thre current working directory, so I have to set my directory for it to work
```{r echo = TRUE, eval = FALSE}

setwd(out_path)

sfiles <- list.files(out_path, pattern = "^radiator_", full.names = TRUE)
sfiles

models <- seq(1, 2, 1)
suffs <- c("1", "2to4")

str(neutral_snps_ds)
class(neutral_snps_ds)
detect_genomic_format(neutral_snps_ds)

# i <- 1
invisible(pblapply(1:length(models), function(i){
  genomic_converter(data = neutral_snps_ds, output = "vcf", filename = paste("ABC_model_", suffs[i], sep = ""), strata = sfiles[i], vcf.metadata = FALSE, vcf.stats = FALSE)
  # any(is.na(tmp$tidy.data$MARKERS))
  # any(is.na(tmp$tidy.data$POP_ID))
  # any(is.na(tmp$tidy.data$INDIVIDUALS))
  # any(is.na(tmp$tidy.data$GT))
  # any(is.na(tmp$tidy.data$REF))
  # any(is.na(tmp$tidy.data$ALT))
  # any(is.na(tmp$tidy.data$GT_VCF))
  # any(is.na(tmp$tidy.data$GT_BIN)) # This column has NAs....
  
  # Works just fine, underscores are automatically changed to dashes
  # read_strata(sfiles[i])
  
}))

# For every run I get an error that a separator is not valid, despite changing all "_" to "-" but the strata file is read in just fine and the package detects the genind class

# I also get an error about NAs created which I think is related to the NA in the POS field in the resulting VCF file, but everything else looks good:
# Warning messages:
# 1: Problem with `mutate()` input `LOCUS`.
# ℹ NAs introduced by coercion
# ℹ Input `LOCUS` is `stringi::stri_join(LOCUS, as.numeric(POS) - 1, sep = "_")`. 
# 2: In stringi::stri_join(LOCUS, as.numeric(POS) - 1, sep = "_") :
#   NAs introduced by coercion

# Looks like this just an error from reading in data with tifyr?
# Resulting VCF file looks different compared to Stacks VCF file, has just genotypes

```

I moved the resulting VCF files out of the ./radiator_genomic_converter* directories and into /media/owner/MYIOPSITTA/R/Origins_Selection/ABC/.

Followed instructions for calculating observed SFSs in this GitHub repo: https://github.com/isaacovercast/easySFS. I used Pycharm to use a virtual environment with Python3.