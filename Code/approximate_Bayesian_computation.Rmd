---
title: "Approximate Bayesian Computation"
author: "Grace Smith-Vidaurre"
date: "September 23, 2020"
output: html_document
---

```{r setup, eval = TRUE, echo = FALSE}

knitr::opts_knit$set(root.dir = "/home/owner/Desktop/GitHub_repos/origins-selection")

```

Purpose: Approximate Bayesian Computation analyses using the multidimensional site frequency spectrum (mSFS), the delimitR package developed by Megan Smith, and fastsimcaol2 for simulating the mSFS under different demographic scenarios. Installed delimitR version 2.0.2 from GitHub: https://github.com/meganlsmith. Had to install dependencies abcrf, sqldf and reticulate. Also installed the package radiator to convert between genind object and VCF format, needed to use Python code in another GitHub repo to get observed SFS data.

Here, 6 models will be run per invasive population with sufficient samples: Uruguay (URY) origin, Northern Argentina (NAR) origin, admixed origin, and these same 3 models but with a longer bottleneck time that goes to the present. 

```{r echo = TRUE, eval = TRUE, message = FALSE}

rm(list = ls())

X <- c("tidyverse", "pbapply", "data.table", "adegenet", "openxlsx", "abcrf", "delimitR")
invisible(lapply(X, library, character.only = TRUE))

# Path to the metadata spreadsheet
xls_path <- "/home/owner/Desktop/MANUSCRIPTS/Origins_Selection/DATA/Metadata_Barcodes"

# Path to Stacks output, including the HWE filtered SNPs in Structure format 
res_path <- "/media/owner/MYIOPSITTA/R/Origins_Selection/stacks"

# Path where ABC files will be written
out_path <- "/media/owner/MYIOPSITTA/R/Origins_Selection/ABC"
 
# Path where population maps written
map_path <- "/media/owner/MYIOPSITTA/R/Origins_Selection/info"

gpath <- "/home/owner/Desktop/MANUSCRIPTS/Origins_Selection/GRAPHICS"

seed <- 401
cores <- parallel::detectCores() - 2

```

Read in metadata.
```{r echo = TRUE, eval = TRUE}

meta_dats <- read.xlsx(file.path(xls_path, "Mymon_RADlibraries_2015_2019_CombinedMetadata.xlsx"))
glimpse(meta_dats)

```

Read in the dataset of pre-processed neutral SNPs from the merged dataset. The number of individuals and loci is documented in BayeScan_post-processing.Rmd.
```{r echo = TRUE, eval = TRUE}

file_nm <- "merged_HWE_missingData_filters_noPosControlDups_neutralSNPs.str"

neutral_snps <- read.structure(file.path(file.path(res_path, "merged"), file_nm), n.ind = 173, n.loc = 320, col.lab = 1, col.pop = 2, row.marknames = 1, onerowperind = FALSE, ask = FALSE, NA.char = "-9")
str(neutral_snps)

str(neutral_snps@tab)

```

# Making population maps to get VCF files via Stacks for ABC modelling

How many invasive populations have sufficient samples (> 4 individuals) for ABC modelling?
```{r echo = TRUE, eval = TRUE}

# Get sampling sites
sites <- sapply(1:nrow(neutral_snps@tab), function(i){
  meta_dats$Site_Code[grep(paste("^", dimnames(neutral_snps@tab)[[1]][i], "$", sep = ""), meta_dats$Sample_Name)]
})
head(sites)

# Get sampling regions
regions <- sapply(1:nrow(neutral_snps@tab), function(i){
  meta_dats$Region[grep(paste("^", dimnames(neutral_snps@tab)[[1]][i], "$", sep = ""), meta_dats$Sample_Name)]
})
head(regions)

# Get native or invasive range
ranges <- sapply(1:nrow(neutral_snps@tab), function(i){
  meta_dats$Population[grep(paste("^", dimnames(neutral_snps@tab)[[1]][i], "$", sep = ""), meta_dats$Sample_Name)]
})
head(ranges)

samps_df <- data.frame(indiv = dimnames(neutral_snps@tab)[[1]]) %>%
  dplyr::mutate(
    site = sites,
    region = regions,
    range = ranges
  )
glimpse(samps_df)

# Only SEVI has too few samples (1 bird) for modelling
samps_df %>%
  filter(range == "Invasive") %>%
  group_by(site) %>%
  dplyr::summarise(
    n_indivs = n_distinct(indiv) 
  )

# Remove the SEVI individual, and all SAR individuals
samps_df <- samps_df %>%
  filter(site != "SEVI") %>%
  filter(region != "Southern Argentina") %>%
  droplevels()
glimpse(samps_df)

samps_df %>%
  pull(region) %>%
  unique()

# Add a column indicating population label for ABC modelling
samps_ABC_df <- samps_df %>%
  dplyr::mutate(
    ABC_pop = region,
    ABC_pop = recode(ABC_pop,
      `Southwestern Uruguay` = "pop0",
      `South Central Uruguay` = "pop0",
      `Spain` = "pop1",
      `Northern United States` = "pop1",
      `Southern United States` = "pop1",
      `Northern Argentina` = "pop2"
    )
  )
glimpse(samps_ABC_df)

# Randomly select 7 birds per region of URY to make models less unbalanced (otherwise 14 NAR and 14 URY individuals per model)
set.seed(seed)
samps_ABC_df <- samps_ABC_df %>%
  filter(grepl("Uruguay", region)) %>%
  group_by(region) %>%
  nest() %>%
  ungroup() %>%
  dplyr::mutate(
    rsamp = map2(data, 7, sample_n, replace = FALSE)
  ) %>%
  select(-data) %>%
  unnest(rsamp) %>%
 # Randomly select 14 birds per FLOR and CNCT
  bind_rows(
    samps_ABC_df %>%
      filter(grepl("CNCT", site)) %>%
      group_by(site) %>%
      nest() %>%
      ungroup() %>%
      dplyr::mutate(
        rsamp = map2(data, 14, sample_n, replace = FALSE)
      ) %>%
      select(-data) %>%
      unnest(rsamp)
  ) %>%
  bind_rows(
    samps_ABC_df %>%
      filter(grepl("FLOR", site)) %>%
      group_by(site) %>%
      nest() %>%
      ungroup() %>%
      dplyr::mutate(
        rsamp = map2(data, 14, sample_n, replace = FALSE)
      ) %>%
      select(-data) %>%
      unnest(rsamp)
  ) %>%
  # Add back all other samples (not URY, FLOR, CNCT)
  bind_rows(
    samps_ABC_df %>%
    filter(!grepl("Uruguay", region) & !grepl("CNCT", site) & !grepl("FLOR", site))
  )

# glimpse(samps_ABC_df)
# View(samps_ABC_df)

# Looks good
samps_ABC_df %>%
  group_by(region, site) %>%
  dplyr::summarise(
    n_indivs = length(indiv)
  )

```

Make a population map per each of the remaining 9 invasive populations. This map will have URY, NAR and INV birds (per each invasive population), but not SAR (to avoid making models too complex). The population map should encode URY individuals as pop1, INV individuals as pop2, and NAR individuals as pop3.
```{r echo = TRUE, eval = TRUE}

# Iterate over invasive range sampling sites
sites <- samps_ABC_df %>%
  filter(range == "Invasive") %>%
  pull(site) %>%
  unique() %>%
  as.character()
sites 
length(sites)  

# x <- 1
# i <- 1
invisible(pblapply(1:length(sites), function(x){
  
  # Initialize file name
  file_nm <- file.path(map_path, paste("popmap_mergedSNPs_ABCmodelling_", sites[x], ".txt", sep = ""))

  # Remove previous versions
  file.remove(file.path(map_path, paste("popmap_mergedSNPs_ABCmodelling_", sites[x], ".txt", sep = "")))
  
  # Get individuals for the given iteration
  tmp_df <- samps_ABC_df %>%
    filter(range == "Native" | site == sites[x]) %>%
    droplevels()
  
  # Order by ABC population
  tmp_df <- tmp_df %>%
    arrange(-desc(ABC_pop))
  # glimpse(tmp_df)
  # View(tmp_df)
    
  # Get individuals
  indivs <- tmp_df %>%
    pull(indiv) %>%
    as.character()
  
  # Get ABC populations
  ABC_pops <- tmp_df %>%
    pull(ABC_pop) %>%
    as.character()
  
  # Iterate over individuals to write out lines to this file
  pblapply(1:length(indivs), function(i){
  
    # Initialize the ABC population for the given sample
    reg <- ABC_pops[i]
  
    # Initialize the suffix to go after the sample name
    # Use ".1" for the PE reads here, since I used only the forward reads when merging SE and PE libraries
    suff <- ifelse(grepl("^SE_", indivs[i]), ".fil.sorted", ".1.sorted")
  
    # If not on the last individual, write out a new line symbol to start the next sample on a new line
    # No suffix after the sample name, to allow Stacks to recognize the paired-end file suffix after kmer_filter (.1.1.fil.fq and .2.2.fil.fq)
    if(i != length(indivs)){
      tmp_line <- paste(paste(paste(indivs[i], suff, sep = ""), reg, sep = "\t"), "\n", sep = "")
    } else {
      tmp_line <- paste(paste(indivs[i], suff, sep = ""), reg, sep = "\t")
    }
  
    if(i == 1){
      cat(tmp_line, file = file_nm)
    } else {
      cat(tmp_line, file = file_nm, append = TRUE)
    }
  
  })
 
}))

# Opened these files in Vim to doublecheck structure, looks good

```

The whitelist of the 320 neutral loci made in "FST_calculations.Rmd" will be used to retain these neutral loci for each VCF file. 

Once these were made, I uploaded to the info folder on Discovery, and made a new script to run Stacks::populations with the whitelist of 320 neutral loci to get VCF files so as to calculate the observed mSFS with easySFS.py. Followed instructions for calculating observed SFSs in this GitHub repo: https://github.com/isaacovercast/easySFS. I used Pycharm to use a virtual environment with Python3.

# Finding optimal bins

From the delimitR manual: "The user must specify how many bins will be used to summarize the SFS. This number should not be greater than the sample size of the population with the fewest samples, as this results in sparse sampling of the SFS. Large values lead to a more complete summary of the data, but also lead to a more sparsely sampled SFS and increased computation times. Users should use exploratory analyses to find the optimal trade-off between computational time and error rates. The user must specify the number of classes that will be used in the binned SFS."

From Smith et al. 2017: "To determine the optimal binning strategy, eight RF classifiers were constructed using the simulated data (i.e., Figure 3, Step 4), one at each level of mSFS coarseness considered here (i.e., 3–10 classes per population). Each classifier was constructed with 500 trees using the R package “ABCRF” (Pudlo et al., 2015), with the bins of the mSFS trea- ted as the predictor variables and the generating model for each simu- lated data set treated as the response variable. At each node in each decision tree, the RF classifier considers a bin of the mSFS and con- structs a binary decision rule based on the number of SNPs in the bin."

Pretty sure this means constructing different priors with the makeprior() function, and then assessing the OOB error over the number of bins used.

Since I'm testing each invasive population separately, with 6 models each, the optimal number of bins should be assessed with each invasive population. 

I tested with BARC and CNCT. Binning the simulated mSFS for BARC with 9 bins took about 30 minutes, but 27 bins for CNCT was taking far too long. Since the trade-off is error rates versus computational time, and in previous exploration with BARC I observed low eeror rates with 5 bins, I will just use an intermediate of 5 bins across all invasive populations.

Create a traits file per invasive population, in delimitR format. The traits file is needed to build each prior with the simulated MSFS, binned into 5 bins across models. 
```{r echo = TRUE, eval = FALSE}

# Iterate over invasive range sampling sites (populations in these analyses)
sites <- samps_ABC_df %>%
  filter(range == "Invasive") %>%
  pull(site) %>%
  unique() %>%
  as.character()
sites 
length(sites) 

# i <- 1
invisible(pblapply(1:length(sites), function(i){
  
  tmp_path <- file.path(out_path, paste("ABC_", sites[i], sep = ""))

  # Make a traits file
  file_nm <- paste("popmap_mergedSNPs_ABCmodelling_", sites[i], ".txt", sep = "")

  pop_map <- read.table(file.path(map_path, file_nm), sep = "\t")
  # str(pop_map)
  # nrow(pop_map)

  # Substitute populations with a zero-based naming system
  pop_map$V2 <- as.numeric(gsub("pop", "", pop_map$V2))
  pop_map$V2 <- pop_map$V2
  
  # Remove previous versions
  file.remove(file.path(tmp_path, "traits.txt"))

  # Print the header
  header <- c(paste(c("traits", "species"), collapse = "\t"), "\n", sep = "")
  cat(header, file = file.path(tmp_path, "traits.txt"), append = TRUE, sep = "")

  # Print rows
  # x <- 1
  invisible(pblapply(1:nrow(pop_map), function(x){
  
    # Initialize file name
    tmp_nm <- file.path(tmp_path, "traits.txt")

    # Add two lines per individual, each represents an allele
    if(x != nrow(pop_map)){
      cat(paste(paste(pop_map[x, 1], "allele_1", sep = "_"), pop_map[x, 2], collapse = "\t"), "\n", file = tmp_nm, sep = "", append = TRUE)
      cat(paste(paste(pop_map[x, 1], "allele_2", sep = "_"), pop_map[x, 2], collapse = "\t"), "\n", file = tmp_nm, sep = "", append = TRUE)
    # Very last individual should not have a new line added after the second allele
    } else {
      cat(paste(paste(pop_map[x, 1], "allele_1", sep = "_"), pop_map[x, 2], collapse = "\t"), "\n", file = tmp_nm, sep = "", append = TRUE)
      cat(paste(paste(pop_map[x, 1], "allele_2", sep = "_"), pop_map[x, 2], collapse = "\t"), file = tmp_nm, sep = "", append = TRUE)
    }

  }))

}))


```

# Build priors with 5 bins

Building priors with an intermediate number of bins for now, since I'm running this on my local machine with fewer computational resources.
```{r echo = TRUE, eval = FALSE}

sites <- samps_ABC_df %>%
  filter(range == "Invasive") %>%
  pull(site) %>%
  unique() %>%
  as.character()
sites 
length(sites) 

nClasses <- 5
obsspecies <- 3 # Number of demes

# Iterate over invasive populations to prepare the observed mSFS 
invisible(pblapply(1:length(sites), function(i){
  
  tmp_path <- file.path(out_path, paste("ABC_", sites[i], sep = ""))
  
  # Get the traits file
  traits <- file.path(tmp_path, "traits.txt")
  
  obsprefix <- paste(sites[i], "model", sep = "_")
  
  # This function assumes the working directory has been set to the folder containing fcs26 output folders
  # Must remove any folder previously made for priors (called "Prior*")
  setwd(tmp_path)

  FullPrior <- makeprior(
    prefix = obsprefix,
    nspec = obsspecies,
    nclasses = nClasses,
    mydir = tmp_path,
    traitsfile = traits,
    threshold = 100,
    thefolder = "Prior",
    ncores = cores
  )
  # CNCT looks good
  str(FullPrior)
  
  saveRDS(FullPrior, file.path(tmp_path, "FullPrior_5bins.RDS"))

  # Remove rows with zero variance, e.g. bins for which no SNPs were ever observed across all models and simulations. 
  ReducedPrior <- Prior_reduced(FullPrior)
  # str(ReducedPrior)

  saveRDS(ReducedPrior, file.path(tmp_path, paste("ReducedPrior_5bins.RDS", sep = "")))
    
  # Remove both priors from global environment to avoid memory issues
  rm(list = c("FullPrior", "ReducedPrior"))
  
}))

# Took 5.5 hours to do the 8 sites after BARC which itself took at least 30 minutes

```


# Train random forests classifiers

Needs to be done on Discovery. I downloaded the trained classifiers to my external hard drive.  


# Predicting the best model per population

To perform prediction, the observed mSFS must be binned in the same way as the RF classifiers. After some troubleshooting, I also realized that the traits file provided during this binning step must reflect the number of samples remaining after downsampling the observed MSFS in easySFS. 

### New trait files for observed mSFS

Here, I made new traits files for the observed MSFS per invasive population. These had no individual ID info, and the number of samples per population remaining after downsampling. 
```{r echo = TRUE, eval = FALSE}

sites <- samps_ABC_df %>%
  filter(range == "Invasive") %>%
  pull(site) %>%
  unique() %>%
  as.character()
sites 
length(sites) 

# i <- 1
# Iterate over invasive populations to prepare the observed mSFS 
invisible(pblapply(1:length(sites), function(i){

  tmp_path <- file.path(out_path, paste("ABC_", sites[i], sep = ""))
  
  # Get the observed mSFS 
  obsmSFS <- readLines(file.path(tmp_path, paste(sites[i], "MSFS.obs", sep = "_")))

  # Get the number of samples
  samp_sizes <- as.numeric(strsplit(strsplit(obsmSFS[2], split = "\t")[[1]][2], split = " ")[[1]])
  # samp_sizes # populations 0, 1, 2

  pops <- seq(0, 2, 1)
  # pops

  # Make a traits file
  file_nm <- paste("traits_observedMSFS_", sites[i], ".txt", sep = "")

  # Remove previous versions if they exist
  if(file.exists(file.path(tmp_path, file_nm))){
    file.remove(file.path(tmp_path, file_nm))
  }

  # Print the header
  header <- c(paste(c("traits", "species"), collapse = "\t"), "\n", sep = "")
  cat(header, file = file.path(tmp_path, file_nm), append = TRUE, sep = "")

  # Make a data frame that will be used to write out info
  traits_df <- rbindlist(pblapply(1:length(pops), function(x){
  
    tmp_df <- data.frame(
      sample = paste(paste(rep("sample", samp_sizes[x]), rep(seq(1, samp_sizes[x]/2, 1), each = 2), sep = "_"), paste("allele", rep(c(1, 2), samp_sizes[x]/2), sep = ""), sep = "_"),
      pop = pops[x]
    )
  
  }))

  # Write out each line of traits_df to the traits file
  invisible(pblapply(1:nrow(traits_df), function(x){

    # Print each line: represents an allele per sample of the downsampled observed MSFS 
    if(x != nrow(traits_df)){
      cat(paste(traits_df$sample[x], traits_df$pop[x], sep = "\t"), "\n", file = file.path(tmp_path, file_nm), sep = "", append = TRUE)
    # Very last individual should not have a new line added after the second allele
    } else {
      cat(paste(traits_df$sample[x], traits_df$pop[x], sep = "\t"), file = file.path(tmp_path, file_nm), sep = "", append = TRUE)
    }

  }))
  
}))

# Opened in Vim, looks good

```


### Prepare observed mSFS

Prediction and then obtaining the posterior probability with a second (regression model) is very intensive, and can't be done on my local machine. I moved this workflow to Discovery, but first have to perform pre-processing of the observed mSFS per invasive population and upload files to Discovery.

Preparing the observed mSFS per invasive population, any previous versions must be deleted (done in code below).
```{r echo = TRUE, eval = FALSE}

sites <- samps_ABC_df %>%
  filter(range == "Invasive") %>%
  pull(site) %>%
  unique() %>%
  as.character()
sites 
length(sites) 

nClasses <- 5
obsspecies <- 3

# Iterate over invasive populations to prepare the observed mSFS 
invisible(pblapply(1:length(sites), function(i){
  
  tmp_path <- file.path(out_path, paste("ABC_", sites[i], sep = ""))
  
  # Get the observed mSFS 
  observedSFS <- file.path(tmp_path, paste(sites[i], "MSFS", sep = "_"))

  # Get the traits file for the observed mSFS
  traits <- file.path(tmp_path, paste("traits_observedMSFS_", sites[i], ".txt", sep = ""))

  # Get the full and reduced priors
  FullPrior <- readRDS(file.path(tmp_path, "FullPrior_5bins.RDS"))
  ReducedPrior <- readRDS(file.path(tmp_path, paste("ReducedPrior_5bins.RDS", sep = "")))
  
  # Remove any previous pre-processed versions of the mSFS
  # Otherwise subsequent pre-processing runs will append more info to these same files
  preproc_obsMSFS <- paste(sites[i], "_MSFS_processed.obs", sep = "")
  binned_obsMSFS <- paste(sites[i], "_MSFS_binned.obs", sep = "")
  
  if(file.exists(file.path(tmp_path, preproc_obsMSFS))){
    file.remove(file.path(tmp_path, preproc_obsMSFS))
  }
  
  if(file.exists(file.path(tmp_path, binned_obsMSFS))){
    file.remove(file.path(tmp_path, binned_obsMSFS))
  }
  
  # Prepare the observed MSFS with the same number of bins used to build priors
  tmp_observed <- prepobserved(
    observed = observedSFS,
    FullPrior = FullPrior,
    ReducedPrior = ReducedPrior,
    nclasses = nClasses,
    npops = obsspecies,
    traitsfile = traits,
    threshold = 100
  )
  
  rm(list = "tmp_observed")

}))


```


In addition to prediction, need to see delimitR manual and referred article to regress over the OOB error rates as a way of assessing confidence in the model.

# Reading in prediction results from Discovery run

```{r echo = TRUE, eval = FALSE}

# Iterate over invasive populations to get prediction results
sites <- samps_ABC_df %>%
  filter(range == "Invasive") %>%
  pull(site) %>%
  unique() %>%
  as.character()
sites 
length(sites) 

# i <- 7
predict_res_list <- invisible(pblapply(1:length(sites), function(i){
  
  tmp_path <- file.path(out_path, paste("ABC_", sites[i], sep = ""))

  # Get the prediction results
  predict_res <- readRDS(file.path(tmp_path, "abcrf_prediction_res.RDS"))
  names(predict_res)
  
  # abcrf predicted model
  # predict_res$predict_res
  
  # abcrf posterior probability from regression RF analysis
  # predict_res$post_prob_res
  
  # Return these results as a list per invasive population
  return(list(
    predicted_model = predict_res$predict_res,
    posterior_prob = predict_res$post_prob_res
  ))
  
  
}))

names(predict_res_list) <- sites

# Check out the results by invasive population. Is the post-proba entry the actual post-probability?

# ?predict.abcrf

# post.prob: post.prob	
# ABC-RF approximations of the posterior probability of the selected model for each observed dataset

# Looks like I didn't need to do that extra regression RF training, it was already done in the prediction call  


# CNCT:

predict_res_list[["CNCT"]]

# BARC:

predict_res_list[["BARC"]]

# FLOR:

predict_res_list[["FLOR"]]

# GRCA:

predict_res_list[["GRCA"]]

# ILLI:

predict_res_list[["ILLI"]]

# MADR:

predict_res_list[["MADR"]]

# MALL:

predict_res_list[["MALL"]]

# WASH:

predict_res_list[["WASH"]]

# ZARA:

predict_res_list[["ZARA"]]

```

Some interesting results here. All populations except ILLI have stronger support for Model 1 (URY origin) than any other model, and ILLI has strongest support for Model 3 (admixture between NAR and URY). But some invasive populations, namely CNCT and especially GRCA, have similar numbers of votes for Model 3 (admixture) as well, indicative of marginal support for admixture. Note that of all invasive populations, GRCA had lower posterior probablity for Model 1 (0.993) compared to 1 for all other populations.

Consider putting these results into a table, with invasive populations in rows and models in columns, also place posterior probability in a last column. Perhaps also include the variance for the posterior probability.

Overall, there is more support for a URY origin for invasive range populations in the U.S. and Spain than a NAR origin.




