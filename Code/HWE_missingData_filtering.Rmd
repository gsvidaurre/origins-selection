---
title: "HWE filtering"
author: "Grace Smith-Vidaurre"
date: "10 September, 2020"
output: html_document
---

```{r setup, eval = TRUE, echo = FALSE}

knitr::opts_knit$set(root.dir = "/home/owner/Desktop/GitHub_repos/origins-selection")

```

Purpose: Make population maps for both sequencing types. All samples will be considered as a single population, and individuals with low coverage should be dropped by commenting out with "#" (see sample_coverage.Rmd). Once done, make denovo_map.pl scripts per sequencing type with the optimized parameters determined above, as well as any other parameters needed per sequencing type, and start Stacks genotyping runs.

```{r echo = TRUE, eval = TRUE, message = FALSE}

rm(list = ls())

X <- c("tidyverse", "pbapply", "data.table", "adegenet", "openxlsx")
invisible(lapply(X, library, character.only = TRUE))

# Path to the metadata spreadsheet that will be updated
xls_path <- "/home/owner/Desktop/MANUSCRIPTS/Origins_Selection/DATA/Metadata_Barcodes"

# Path to Stacks output 
res_path <- "/media/owner/MYIOPSITTA/R/Origins_Selection/stacks"

```

Read in metadata.
```{r echo = TRUE, eval = TRUE}

meta_dats <- read.xlsx(file.path(xls_path, "Mymon_RADlibraries_2015_2019_CombinedMetadata.xlsx"))
glimpse(meta_dats)

```

Read in the populations.sumstats.tsv files with HWE p-values. Significant p-values (< 0.05) mean a locus was out of equilibrium. Here, search for loci that were out of HWE in 2 population contrasts for the SE SNPs, and 3 populations for the PE and merged SNPs. Convert zeroes to -9999 to indicate missing data, and remove suffixes after sample names.

### SE SNPs, HWE filtering

In this .tsv file, the first 3 lines contain the population IDs used for HWE, each followed by all individuals in that population. Then the next line contains column headers, then follow all the lines with data. The population and column header lines all start with "#". Note that each row is actually a locus per population, so each locus is repeated for the given number of populations used in the HWE contrasts.
```{r echo = TRUE, eval = FALSE}

# Read the first lines of metadata 
meta <- readLines(file.path(file.path(res_path, "single_end/HWE"), "populations.sumstats.tsv"), n = 4)
# meta

# Get the populations
pops <- meta[grep("\tSE", meta)]
# pops

# Then get the column headers
col_nms <- meta[grep("# Locus ID", meta)]
# col_nms

# Split out the column headers into a vector, one element per header
col_nms <- strsplit(col_nms, split = "\t")[[1]]

# Substitute symbols that will interfere with becoming column headers
col_nms <- gsub("# ", "", col_nms)
# col_nms

col_nms <- gsub("[[:space:]]|-", "_", col_nms)
col_nms

# Get the data lines
sumstats_se <- read.table(file.path(file.path(res_path, "single_end/HWE"), "populations.sumstats.tsv"), sep = "\t", header = FALSE)
glimpse(sumstats_se)

# Add back headers
names(sumstats_se) <- col_nms
glimpse(sumstats_se)

# Find loci out of HWE using alpha of 0.05
# Here, create a unique locus ID that contains the locus number plus the basepair of the SNP within the RAD-tag, to facilitate filtering the Structure file below, which has the locus IDs in this format
# Then summarize the resulting data to find loci present 2 or more times
HW_disequil <- sumstats_se %>%
  # Create a new locus ID column
  dplyr::mutate(
    # Col is the nucleotide site within the catalog locus, not that this is a zero-based offset, such that the first nucleotide is 0
    locus_bp_ID = paste(Locus_ID, Col, sep = "_")
  ) %>%
  filter(HWE_P_value <= 0.05) %>%
  droplevels() %>%
  group_by(locus_bp_ID) %>%
  dplyr::summarise(
    n_pops = n_distinct(Pop_ID)
  ) %>%
  filter(n_pops == 3) %>% # Only get loci out of HWE in all 3 populations
  pull(locus_bp_ID)

head(HW_disequil)

# 4518 loci out of HWE
length(HW_disequil)

# 40129 loci total
sumstats_se %>%
  dplyr::mutate(
    locus_bp_ID = paste(Locus_ID, Col, sep = "_")
  ) %>%
  pull(locus_bp_ID) %>%
  unique() %>%
  length()

# If the total number of loci from the Stacks::populations run with a single population is the same as the HWE run here, then 39611 loci should remain after removing the HW_disequil loci
sumstats_se %>%
  dplyr::mutate(
    locus_bp_ID = paste(Locus_ID, Col, sep = "_")
  ) %>%
  pull(locus_bp_ID) %>%
  unique() %>%
  length() - length(HW_disequil)

# Read in the full set of loci obtained from the non-HWE run with all individuals considered in the same population
# Here using the Structure file output by Stacks, which will be filtered and then used to make Bayescan input files
# This file has two initial lines of metadata: the first line starts with "#" and contains a tag with the version of Stacks used as well as the date the file was created 
# Here reading this file in as a table to get the dimensions (two rows per individual because these are diploid, and number of columns is the number of loci)
struc <- read.table(file.path(file.path(res_path, "single_end"), "populations.structure"), skip = 2)
# glimpse(struc)
dim(struc) # 278 rows, 34246 columns
head(struc[, 1]) # Sample names contained in the first column
head(struc[, 2]) # Population ID (single population) contained in second column

# The number of loci in this file is not the same as when running Stacks::populations for HWE. This is because the rules given to populations (-r 0.8 for instance) should yield different results when samples are considered as a single or multiple populations. Regardless, can still search for the loci out of HWE in the single populations output file, and remove them if they are present

# Get the column headers

# Read the first lines of metadata 
meta <- readLines(file.path(file.path(res_path, "single_end"), "populations.structure"), n = 2)
# meta

# Then get the column headers
col_nms <- meta[-grep("#", meta)]
# col_nms

# Split out the column headers into a vector, one element per header
col_nms <- strsplit(col_nms, split = "\t")[[1]]
head(col_nms)
length(col_nms)

# How to filter this data and then write out in Structure format again??
# Could iterate over each row of the data frame, remove the HWE_disequil loci, then print the filtered tab-separated line (preceded by the sample name) to a new file with the extension .str
# If proceed with this plan, then will need to print the Structure header (two blank spaces, then loci names, all tab-separated) prior to printing the data lines inside a loop
names(struc) <- col_nms
names(struc)[1:2] <- c("indiv", "pop") # Fix the first two column names for the loop below
glimpse(struc[, 1:10]) # First column contains the sample name, second column is the population identifier

# Filter the genotypes data frame to remove loci in HWE_disequil
head(HW_disequil)
head(names(struc))

# Not sure why grep fails here...
# struc2 <- struc[, -grep(paste(paste("^", HW_disequil, "$", sep = ""), collapse = "|"), names(struc))]
struc2 <- struc[, -which(names(struc) %in% HW_disequil)]
dim(struc2) # 278 rows = 139 diploid individuals, 33728 - 2 = 33726 loci remain
unique(struc2$pop) # A single population

rm(list = "struc") # Remove this object as it takes up memory and won't be used in this format

# Convert all zeroes (missing data) to -9999 (more compatible with Structure)
# which(struc2 == 0)
struc2[struc2 == 0] <- -9999

# Filter column names as well
col_nms2 <- col_nms[-which(col_nms %in% HW_disequil)]
length(col_nms2)

# Checking, all remaining column names are not in the list of loci that should have been dropped, looks good 
# all(!names(struc2) %in% HW_disequil)
# all(!col_nms2 %in% HW_disequil)

# Remove suffixes in sample names
struc2$indiv <- gsub(".fil", "", as.character(struc2$indiv))

# Create a new Structure file, then proceed with the plan above
# Opened this file periodically in Vim to doublecheck structure
# Used :set list in Vim to see invisible characters
file_nm <- "single_end_HWE_filter.str"

# Remove previous versions
file.remove(file.path(file.path(res_path, "single_end"), file_nm))

# Initialize the Structure header, with the two tab symbols at the beginning and a new line symbol at the end
header <- c("\t", "\t", paste(paste(col_nms2[-grep("^$", col_nms2)], collapse = " "), "\n", sep = ""))
# header

# Print the Structure header
cat(header, file = file.path(file.path(res_path, "single_end"), file_nm), append = TRUE)

# Print each line of the filtered data frame above
invisible(pblapply(1:nrow(struc2), function(i){

  cat(c(struc2$indiv[i], "\t", struc2$pop[i], "\t", paste(paste(struc2[i, -grep("^indiv$|^pop$", names(struc2))], collapse = " "), "\n", sep = "")), file = file.path(file.path(res_path, "single_end"), file_nm), append = TRUE)

}))

# Opened the file in Vim to check out results, looks great

```

### SE SNPs, additional filtering

Filter out individuals with > 80% missing data, and any loci with > 80% missing data for any sampling location.
```{r echo = TRUE, eval = FALSE}

# Find the percentage missing data per individual
names(struc2)[1:10]

indivs <- unique(struc2$indiv)
indivs

# i <- 1
# z <- 1
i2rem <- rbindlist(pblapply(1:length(indivs), function(i){
  
  # Get each allele per locus per individual
  # Each individual has two rows, one per allele
  tmp <- struc2[grep(paste("^", indivs[i], "$", sep = ""), struc2$indiv), -grep("indiv|pop", names(struc2))]
  # dim(tmp)
  
  # Find the percentage of missing data per diploid row
  perc <- unlist(lapply(1:nrow(tmp), function(z){
    (length(which(tmp[z, ] == -9999))/length(tmp[z, ]))*100
  }))
  
  # if(any(perc) >= 80){
    # return(indivs[i])
  # } else {
    return(data.frame(indiv = paste(indivs[i], c(1, 2), sep = "_"), perc_miss = perc))
  # }
  
}))
glimpse(i2rem)

range(i2rem$perc_miss)
hist(i2rem$perc_miss) # Natural breaks at 10% missing data (very conservative), and 30% (less conservative)

# If I did filter out individuals with > 10% missing data, would any CT birds be left?
# Yep, there would be more than 10 left
i2rem %>%
  dplyr::mutate(
    indiv = gsub("_1|_2", "", indiv)
  ) %>%
  filter(perc_miss <= 10) %>%
  pull(indiv) %>%
  unique() %>%
  as.character()

# Which individuals have more than 10% missing data?
# A LOT of CT individuals....
is <- i2rem %>%
  dplyr::mutate(
    indiv = gsub("_1|_2", "", indiv)
  ) %>%
  filter(perc_miss > 10) %>%
  pull(indiv) %>%
  unique() %>%
  as.character()

# Remove these 35 individuals, a very conservative filter on missing data!
is

# Iterate over sampling locations, and within samples per location, find loci with > 80% missing data
sites <- unique(meta_dats$Site_Code)
sites

loci <- names(struc2)[-grep("indiv|pop", names(struc2))]
head(loci)

# i <- 1
# z <- 1
l2rem_df <- rbindlist(pblapply(1:length(sites), function(i){
  
  # Subset the Structure object by individuals at the given site
  tmp_indivs <- meta_dats %>%
    filter(Sequencing_Type == "Single-end") %>%
    filter(Site_Code == sites[i]) %>%
    pull(Sample_Name)
  
  tmp_str <- struc2 %>%
    filter(indiv %in% tmp_indivs)
  # dim(tmp_str)

  l2rem <- rbindlist(lapply(1:length(loci), function(z){
    SNPs <- tmp_str[, grep(paste("^", loci[z], "$", sep = ""), names(tmp_str))]
    perc <- (length(which(SNPs == -9999))/length(SNPs))*100
    return(data.frame(site = sites[i], locus_ID = loci[z], perc_miss = perc))
  }))
  
  return(l2rem)
  
}))

glimpse(l2rem_df)

saveRDS(l2rem_df, file.path(file.path(res_path, "single_end"), "SE_l2rem_df.RDS"))

# Distribution of missing data across loci when evaluated by site?
# Find good natural breaks and use this to set a threshold for removal of loci with the highest amounts of missing data
l2rem_df <- readRDS(file.path(file.path(res_path, "single_end"), "SE_l2rem_df.RDS"))

# Histogram of non-zero missing data percentages
# 20% is a good natural break
l2rem_df %>%
  filter(perc_miss > 0) %>%
  pull(perc_miss) %>%
  hist()

l2rem <- l2rem_df %>%
  filter(perc_miss >= 20) %>%
  pull(locus_ID) %>%
  unique() %>%
  as.character()

# Wow that halves the number of loci for the SE SNP dataset
length(l2rem)
head(l2rem)

# Remove the individuals and loci determined to have high levels of missing data
struc3 <- struc2 %>%
  filter(!indiv %in% is) %>%
  dplyr::select(-c(all_of(l2rem))) %>%
  droplevels()

dim(struc2) # 139 individuals, 33726 loci
dim(struc3) # 210 rows = 105 diploid individuals, 14383 - 2 = 14381 loci

# Write out a new file as above for the SE libraries, then remove the struc2 and 3 objects

# Create a new Structure file, then proceed with the plan above
# Opened this file periodically in Vim to doublecheck structure
# Used :set list in Vim to see invisible characters
file_nm <- "single_end_HWE_missingData_filters.str"

# Remove previous versions
file.remove(file.path(file.path(res_path, "single_end"), file_nm))

# Initialize the Structure header, with the two tab symbols at the beginning and a new line symbol at the end
header <- c("\t", "\t", paste(paste(names(struc3)[-grep("^indiv$|^pop$", names(struc3))], collapse = " "), "\n", sep = ""))
# header

# Print the Structure header
cat(header, file = file.path(file.path(res_path, "single_end"), file_nm), append = TRUE)

# Print each line of the filtered data frame above
invisible(pblapply(1:nrow(struc3), function(i){

  cat(c(struc3$indiv[i], "\t", struc3$pop[i], "\t", paste(paste(struc3[i, -grep("^indiv$|^pop$", names(struc3))], collapse = " "), "\n", sep = "")), file = file.path(file.path(res_path, "single_end"), file_nm), append = TRUE)

}))

# Opened the file in Vim to check out results, looks good
rm(list = c("struc2", "struc3"))

```

### PE SNPs

4 populations, so 5 lines of metadata. No suffixes to remove in sample names.
```{r echo = TRUE, eval = FALSE}

# Read the first lines of metadata 
meta <- readLines(file.path(file.path(res_path, "paired_end/HWE"), "populations.sumstats.tsv"), n = 5)
# meta

# Get the populations
pops <- meta[grep("\tPE", meta)]
# pops

# Then get the column headers
col_nms <- meta[grep("# Locus ID", meta)]
col_nms

# Split out the column headers into a vector, one element per header
col_nms <- strsplit(col_nms, split = "\t")[[1]]

# Substitute symbols that will interfere with becoming column headers
col_nms <- gsub("# ", "", col_nms)
col_nms

col_nms <- gsub("[[:space:]]|-", "_", col_nms)
col_nms

# Get the data lines
sumstats_pe <- read.table(file.path(file.path(res_path, "paired_end/HWE"), "populations.sumstats.tsv"), sep = "\t", header = FALSE)
# glimpse(sumstats_pe)

# Add back headers
names(sumstats_pe) <- col_nms
glimpse(sumstats_pe[, 1:10])

# Find loci out of HWE using alpha of 0.05
# Here, create a unique locus ID that contains the locus number plus the basepair of the SNP within the RAD-tag, to facilitate filtering the Structure file below, which has the locus IDs in this format
# Then summarize the resulting data to find loci present 2 or more times
HW_disequil <- sumstats_pe %>%
  # Create a new locus ID column
  dplyr::mutate(
    # Col is the nucleotide site within the catalog locus, not that this is a zero-based offset, such that the first nucleotide is 0
    locus_bp_ID = paste(Locus_ID, Col, sep = "_")
  ) %>%
  filter(HWE_P_value <= 0.05) %>%
  droplevels() %>%
  group_by(locus_bp_ID) %>%
  dplyr::summarise(
    n_pops = n_distinct(Pop_ID)
  ) %>%
  filter(n_pops > 1) %>%
  pull(locus_bp_ID)

head(HW_disequil)

# 307 loci out of HWE
length(HW_disequil)

# 36106 loci total
sumstats_pe %>%
  dplyr::mutate(
    locus_bp_ID = paste(Locus_ID, Col, sep = "_")
  ) %>%
  pull(locus_bp_ID) %>%
  unique() %>%
  length()

# If the total number of loci from the Stacks::populations run with a single population is the same as the HWE run here, then 35799 loci should remain after removing the HW_disequil loci
sumstats_pe %>%
  dplyr::mutate(
    locus_bp_ID = paste(Locus_ID, Col, sep = "_")
  ) %>%
  pull(locus_bp_ID) %>%
  unique() %>%
  length() - length(HW_disequil)

# Read in the full set of loci obtained from the non-HWE run with all individuals considered in the same population
# Here using the Structure file output by Stacks, which will be filtered and then used to make Bayescan input files
# This file has two initial lines of metadata: the first line starts with "#" and contains a tag with the version of Stacks used as well as the date the file was created 
# Here reading this file in as a table to get the dimensions (two rows per individual because these are diploid, and number of columns is the number of loci)
struc <- read.table(file.path(file.path(res_path, "paired_end"), "populations.structure"), skip = 2)
# glimpse(struc)
dim(struc) # 286 rows, 6443 columns
# head(struc[, 1]) # Sample names contained in the first column
 
# The number of loci in this file is not the same as when running Stacks::populations for HWE. This is because the rules given to populations (-r 0.8 for instance) should yield different results when samples are considered as a single or multiple populations. Regardless, can still search for the loci out of HWE in the single populations output file, and remove them if they are present

# Get the column headers

# Read the first lines of metadata 
meta <- readLines(file.path(file.path(res_path, "paired_end"), "populations.structure"), n = 2)
# meta

# Then get the column headers
col_nms <- meta[-grep("#", meta)]
# col_nms

# Split out the column headers into a vector, one element per header
col_nms <- strsplit(col_nms, split = "\t")[[1]]
head(col_nms)

# How to filter this data and then write out in Structure format again??
# Could iterate over each row of the data frame, remove the HWE_disequil loci, then print the filtered tab-separated line (preceded by the sample name) to a new file with the extension .str
# If proceed with this plan, then will need to print the Structure header (two blank spaces, then loci names, all tab-separated) prior to printing the data lines inside a loop
names(struc) <- col_nms
names(struc)[1:2] <- c("indiv", "pop") # Fix the first two column names for the loop below
glimpse(struc[, 1:10]) # First column contains the sample name, second column is the population identifier

# Filter the genotypes data frame to remove loci in HWE_disequil
struc2 <- struc[, -which(names(struc) %in% HW_disequil)]
dim(struc2) # 286 rows = 143 diploid individuals, 6387 - 2 = 6385 loci remain
unique(struc2$pop) # A single population

# Convert all zeroes (missing data) to -9999 (more compatible with Structure)
# which(struc2 == 0)
struc2[struc2 == 0] <- -9999

# Filter column names as well
col_nms2 <- col_nms[-which(col_nms %in% HW_disequil)]
length(col_nms2)

# Checking, all remaining column names are not in the list of loci that should have been dropped, looks good 
# all(!names(struc2) %in% HW_disequil)
# all(!col_nms2 %in% HW_disequil)

rm(list = "struc") # Remove this object as it takes up memory and won't be used in this format

# Create a new Structure file, then proceed with the plan above
# Opened this file periodically in Vim to doublecheck structure
# Used :set list in Vim to see invisible characters
file_nm <- "paired_end_HWE_filter.str"

# Remove previous versions
file.remove(file.path(file.path(res_path, "paired_end"), file_nm))

# Initialize the Structure header, with the two tab symbols at the beginning and a new line symbol at the end
header <- c("\t", "\t", paste(paste(col_nms2[-grep("^$", col_nms2)], collapse = " "), "\n", sep = ""))
# header

# Print the Structure header
cat(header, file = file.path(file.path(res_path, "paired_end"), file_nm), append = TRUE)

# Print each line of the filtered data frame above
invisible(pblapply(1:nrow(struc2), function(i){

  cat(c(as.character(struc2$indiv[i]), "\t", struc2$pop[i], "\t", paste(paste(struc2[i, -grep("^indiv$|^pop$", names(struc2))], collapse = " "), "\n", sep = "")), file = file.path(file.path(res_path, "paired_end"), file_nm), append = TRUE)

}))

# Opened the file in Vim to check out results, looks great
rm(list = "struc2")

```

### PE SNPs, additional filtering

Filter out individuals with > 80% missing data, and any loci with > 80% missing data for any sampling location.
```{r echo = TRUE, eval = FALSE}

# Find the percentage missing data per individual
names(struc2)[1:10]

indivs <- unique(as.character(struc2$indiv))
indivs

# i <- 1
# z <- 1
i2rem <- rbindlist(pblapply(1:length(indivs), function(i){
  
  # Get each allele per locus per individual
  # Each individual has two rows, one per allele
  tmp <- struc2[grep(paste("^", indivs[i], "$", sep = ""), struc2$indiv), -grep("indiv|pop", names(struc2))]
  # dim(tmp)
  
  # Find the percentage of missing data per diploid row
  perc <- unlist(lapply(1:nrow(tmp), function(z){
    (length(which(tmp[z, ] == -9999))/length(tmp[z, ]))*100
  }))
  
  # if(any(perc) >= 80){
    # return(indivs[i])
  # } else {
    return(data.frame(indiv = paste(indivs[i], c(1, 2), sep = "_"), perc_miss = perc))
  # }
  
}))
glimpse(i2rem)

range(i2rem$perc_miss)
hist(i2rem$perc_miss) # Natural breaks at 10% missing data (very conservative), and 30% (less conservative)

# If I did filter out individuals with > 10% missing data, would any CT birds be left?
# Yep, there would be about 5 left and 82 birds left total
i2rem %>%
  dplyr::mutate(
    indiv = gsub("_1|_2", "", indiv)
  ) %>%
  filter(perc_miss <= 10) %>%
  pull(indiv) %>%
  unique() %>%
  as.character()

# Which individuals have more than 10% missing data?
# A LOT of individuals....
is <- i2rem %>%
  dplyr::mutate(
    indiv = gsub("_1|_2", "", indiv)
  ) %>%
  filter(perc_miss > 10) %>%
  pull(indiv) %>%
  unique() %>%
  as.character()

# Remove these 61 individuals, a very conservative filter on missing data!
is

# Iterate over sampling locations, and within samples per location, find loci with > 80% missing data
sites <- meta_dats %>%
  filter(Sequencing_Type == "Paired-end") %>%
  pull(Site_Code) %>%
  unique()
sites

loci <- names(struc2)[-grep("indiv|pop", names(struc2))]
head(loci)

# i <- 1
# z <- 1
l2rem_df <- rbindlist(pblapply(1:length(sites), function(i){
  
  # Subset the Structure object by individuals at the given site
  tmp_indivs <- meta_dats %>%
    filter(Sequencing_Type == "Paired-end") %>%
    filter(Site_Code == sites[i]) %>%
    pull(Sample_Name)
  
  tmp_str <- struc2 %>%
    filter(indiv %in% tmp_indivs)
  # dim(tmp_str)

  l2rem <- rbindlist(lapply(1:length(loci), function(z){
    SNPs <- tmp_str[, grep(paste("^", loci[z], "$", sep = ""), names(tmp_str))]
    perc <- (length(which(SNPs == -9999))/length(SNPs))*100
    return(data.frame(site = sites[i], locus_ID = loci[z], perc_miss = perc))
  }))
  
  return(l2rem)
  
}))

glimpse(l2rem_df)

saveRDS(l2rem_df, file.path(file.path(res_path, "paired_end"), "PE_l2rem_df.RDS"))

# Distribution of missing data across loci when evaluated by site?
# Find good natural breaks and use this to set a threshold for removal of loci with the highest amounts of missing data
l2rem_df <- readRDS(file.path(file.path(res_path, "paired_end"), "PE_l2rem_df.RDS"))

# Histogram of non-zero missing data percentages
# 50% is a good natural break here
l2rem_df %>%
  filter(perc_miss > 0) %>%
  pull(perc_miss) %>%
  hist()

# Using 20% only leaves 2 loci...and using 35% leaves only a little over a hundred loci
l2rem <- l2rem_df %>%
  filter(perc_miss > 50) %>%
  pull(locus_ID) %>%
  unique() %>%
  as.character()

# Wow that is about half of the loci for the PE SNP dataset
length(l2rem)
head(l2rem)

# Remove the individuals and loci determined to have high levels of missing data
struc3 <- struc2 %>%
  dplyr::mutate(
    indiv = as.character(indiv)
  ) %>%
  filter(!indiv %in% is) %>%
  dplyr::select(-c(all_of(l2rem))) %>%
  droplevels()

dim(struc2) # 143 individuals, 6385 loci
dim(struc3) # 166 rows = 83 diploid individuals, 2942 - 2 = 2940 loci

# Write out a new file as above for the SE libraries, then remove the struc2 and 3 objects

# Create a new Structure file, then proceed with the plan above
# Opened this file periodically in Vim to doublecheck structure
# Used :set list in Vim to see invisible characters
file_nm <- "paired_end_HWE_missingData_filters.str"

# Remove previous versions
file.remove(file.path(file.path(res_path, "paired_end"), file_nm))

# Initialize the Structure header, with the two tab symbols at the beginning and a new line symbol at the end
header <- c("\t", "\t", paste(paste(names(struc3)[-grep("^indiv$|^pop$", names(struc3))], collapse = " "), "\n", sep = ""))
# header

# Print the Structure header
cat(header, file = file.path(file.path(res_path, "paired_end"), file_nm), append = TRUE)

# Print each line of the filtered data frame above
invisible(pblapply(1:nrow(struc3), function(i){

  cat(c(struc3$indiv[i], "\t", struc3$pop[i], "\t", paste(paste(struc3[i, -grep("^indiv$|^pop$", names(struc3))], collapse = " "), "\n", sep = "")), file = file.path(file.path(res_path, "paired_end"), file_nm), append = TRUE)

}))

# Opened the file in Vim to check out results, looks good
rm(list = c("struc2", "struc3"))

```

### Merged SNPs

4 populations, so 5 lines of metadata here too.
```{r echo = TRUE, eval = FALSE}

# Read the first lines of metadata 
meta <- readLines(file.path(file.path(res_path, "merged/HWE"), "populations.sumstats.tsv"), n = 5)
# meta

# Get the populations
pops <- meta[grep("\tPE|\tSE", meta)]
# pops

# Then get the column headers
col_nms <- meta[grep("# Locus ID", meta)]
col_nms

# Split out the column headers into a vector, one element per header
col_nms <- strsplit(col_nms, split = "\t")[[1]]

# Substitute symbols that will interfere with becoming column headers
col_nms <- gsub("# ", "", col_nms)
col_nms

col_nms <- gsub("[[:space:]]|-", "_", col_nms)
col_nms

# Get the data lines
sumstats_merge <- read.table(file.path(file.path(res_path, "merged/HWE"), "populations.sumstats.tsv"), sep = "\t", header = FALSE)
# glimpse(sumstats_merge)

# Add back headers
names(sumstats_merge) <- col_nms
glimpse(sumstats_merge[, 1:10])

# Find loci out of HWE using alpha of 0.05
# Here, create a unique locus ID that contains the locus number plus the basepair of the SNP within the RAD-tag, to facilitate filtering the Structure file below, which has the locus IDs in this format
# Then summarize the resulting data to find loci present 2 or more times
HW_disequil <- sumstats_merge %>%
  # Create a new locus ID column
  dplyr::mutate(
    # Col is the nucleotide site within the catalog locus, not that this is a zero-based offset, such that the first nucleotide is 0
    locus_bp_ID = paste(Locus_ID, Col, sep = "_")
  ) %>%
  filter(HWE_P_value <= 0.05) %>%
  droplevels() %>%
  group_by(locus_bp_ID) %>%
  dplyr::summarise(
    n_pops = n_distinct(Pop_ID)
  ) %>%
  filter(n_pops > 1) %>%
  pull(locus_bp_ID)

head(HW_disequil)

# 153 loci out of HWE
length(HW_disequil)

# 1254 loci total
sumstats_merge %>%
  dplyr::mutate(
    locus_bp_ID = paste(Locus_ID, Col, sep = "_")
  ) %>%
  pull(locus_bp_ID) %>%
  unique() %>%
  length()

# If the total number of loci from the Stacks::populations run with a single population is the same as the HWE run here, then 1101 loci should remain after removing the HW_disequil loci
sumstats_merge %>%
  dplyr::mutate(
    locus_bp_ID = paste(Locus_ID, Col, sep = "_")
  ) %>%
  pull(locus_bp_ID) %>%
  unique() %>%
  length() - length(HW_disequil)

# Read in the full set of loci obtained from the non-HWE run with all individuals considered in the same population
# Here using the Structure file output by Stacks, which will be filtered and then used to make Bayescan input files
# This file has two initial lines of metadata: the first line starts with "#" and contains a tag with the version of Stacks used as well as the date the file was created 
# Here reading this file in as a table to get the dimensions (two rows per individual because these are diploid, and number of columns is the number of loci)
struc <- read.table(file.path(file.path(res_path, "merged"), "populations.structure"), skip = 2)
# glimpse(struc)
dim(struc) # 564 rows, 1022 columns
# head(struc[, 1]) # Sample names contained in the first column
 
# The number of loci in this file is not the same as when running Stacks::populations for HWE. This is because the rules given to populations (-r 0.8 for instance) should yield different results when samples are considered as a single or multiple populations. Regardless, can still search for the loci out of HWE in the single populations output file, and remove them if they are present

# Get the column headers

# Read the first lines of metadata 
meta <- readLines(file.path(file.path(res_path, "merged"), "populations.structure"), n = 2)
# meta

# Then get the column headers
col_nms <- meta[-grep("#", meta)]
# col_nms

# Split out the column headers into a vector, one element per header
col_nms <- strsplit(col_nms, split = "\t")[[1]]
head(col_nms)

# How to filter this data and then write out in Structure format again??
# Could iterate over each row of the data frame, remove the HWE_disequil loci, then print the filtered tab-separated line (preceded by the sample name) to a new file with the extension .str
# If proceed with this plan, then will need to print the Structure header (two blank spaces, then loci names, all tab-separated) prior to printing the data lines inside a loop
names(struc) <- col_nms
names(struc)[1:2] <- c("indiv", "pop") # Fix the first two column names for the loop below
glimpse(struc[, 1:10]) # First column contains the sample name, second column is the population identifier

# Filter the genotypes data frame to remove loci in HWE_disequil
struc2 <- struc[, -which(names(struc) %in% HW_disequil)]
dim(struc2) # 564 rows = 282 diploid individuals, 871 - 2 = 869 remaining loci
unique(struc2$pop) # A single population

rm(list = "struc") # Remove this object as it takes up memory and won't be used in this format

# Convert all zeroes (missing data) to -9999 (more compatible with Structure)
# which(struc2 == 0)
struc2[struc2 == 0] <- -9999

# Filter column names as well
col_nms2 <- col_nms[-which(col_nms %in% HW_disequil)]
length(col_nms2)

# Checking, all remaining column names are not in the list of loci that should have been dropped, looks good 
# all(!names(struc2) %in% HW_disequil)
# all(!col_nms2 %in% HW_disequil)

# Remove suffixes in sample names
struc2$indiv <- gsub(".fil.sorted|.1.sorted", "", as.character(struc2$indiv))

# Create a new Structure file, then proceed with the plan above
# Opened this file periodically in Vim to doublecheck structure
# Used :set list in Vim to see invisible characters
file_nm <- "merged_HWE_filter.str"

# Remove previous versions
file.remove(file.path(file.path(res_path, "merged"), file_nm))

# Initialize the Structure header, with the two tab symbols at the beginning and a new line symbol at the end
header <- c("\t", "\t", paste(paste(col_nms2[-grep("^$", col_nms2)], collapse = " "), "\n", sep = ""))
# header

# Print the Structure header
cat(header, file = file.path(file.path(res_path, "merged"), file_nm), append = TRUE)

# Print each line of the filtered data frame above
invisible(pblapply(1:nrow(struc2), function(i){

  cat(c(as.character(struc2$indiv[i]), "\t", struc2$pop[i], "\t", paste(paste(struc2[i, -grep("^indiv$|^pop$", names(struc2))], collapse = " "), "\n", sep = "")), file = file.path(file.path(res_path, "merged"), file_nm), append = TRUE)

}))

# Opened the file in Vim to check out results, looks great
rm(list = "struc2")

```

### Merged SNPs, additional filtering

Filter out individuals with > 80% missing data, and any loci with > 80% missing data for any sampling location.
```{r echo = TRUE, eval = FALSE}

# Find the percentage missing data per individual
names(struc2)[1:10]

indivs <- unique(as.character(struc2$indiv))
indivs

# i <- 1
# z <- 1
i2rem <- rbindlist(pblapply(1:length(indivs), function(i){
  
  # Get each allele per locus per individual
  # Each individual has two rows, one per allele
  tmp <- struc2[grep(paste("^", indivs[i], "$", sep = ""), struc2$indiv), -grep("indiv|pop", names(struc2))]
  # dim(tmp)
  
  # Find the percentage of missing data per diploid row
  perc <- unlist(lapply(1:nrow(tmp), function(z){
    (length(which(tmp[z, ] == -9999))/length(tmp[z, ]))*100
  }))
  
  # if(any(perc) >= 80){
    # return(indivs[i])
  # } else {
    return(data.frame(indiv = paste(indivs[i], c(1, 2), sep = "_"), perc_miss = perc))
  # }
  
}))
glimpse(i2rem)

range(i2rem$perc_miss)
hist(i2rem$perc_miss) # Natural breaks at 10% missing data (very conservative), and 30% (less conservative)

# If I did filter out individuals with > 10% missing data, would any CT birds be left?
# Yep, there would be about 5 left in PE, more in SE, and 215 birds left total
i2rem %>%
  dplyr::mutate(
    indiv = gsub("_1|_2", "", indiv)
  ) %>%
  filter(perc_miss <= 10) %>%
  pull(indiv) %>%
  unique() %>%
  as.character()

# Which individuals have more than 10% missing data?
# A lot of individuals....
is <- i2rem %>%
  dplyr::mutate(
    indiv = gsub("_1|_2", "", indiv)
  ) %>%
  filter(perc_miss > 10) %>%
  pull(indiv) %>%
  unique() %>%
  as.character()

# Remove these 67 individuals, a very conservative filter on missing data!
is

# Iterate over sampling locations, and within samples per location, find loci with > 80% missing data
sites <- meta_dats %>%
  pull(Site_Code) %>%
  unique()
sites

loci <- names(struc2)[-grep("indiv|pop", names(struc2))]
head(loci)

# i <- 1
# z <- 1
l2rem_df <- rbindlist(pblapply(1:length(sites), function(i){
  
  # Subset the Structure object by individuals at the given site
  tmp_indivs <- meta_dats %>%
    filter(Site_Code == sites[i]) %>%
    pull(Sample_Name)
  
  tmp_str <- struc2 %>%
    filter(indiv %in% tmp_indivs)
  # dim(tmp_str)

  l2rem <- rbindlist(lapply(1:length(loci), function(z){
    SNPs <- tmp_str[, grep(paste("^", loci[z], "$", sep = ""), names(tmp_str))]
    perc <- (length(which(SNPs == -9999))/length(SNPs))*100
    return(data.frame(site = sites[i], locus_ID = loci[z], perc_miss = perc))
  }))
  
  return(l2rem)
  
}))

glimpse(l2rem_df)

saveRDS(l2rem_df, file.path(file.path(res_path, "merged"), "merged_l2rem_df.RDS"))

# Distribution of missing data across loci when evaluated by site?
# Find good natural breaks and use this to set a threshold for removal of loci with the highest amounts of missing data
l2rem_df <- readRDS(file.path(file.path(res_path, "merged"), "merged_l2rem_df.RDS"))

# Histogram of non-zero missing data percentages
# 30% is a good natural break here
l2rem_df %>%
  filter(perc_miss > 0) %>%
  pull(perc_miss) %>%
  hist()

l2rem <- l2rem_df %>%
  filter(perc_miss > 30) %>%
  pull(locus_ID) %>%
  unique() %>%
  as.character()

# Also about half of the loci for the merged SNP dataset
length(l2rem)
head(l2rem)
ncol(struc2)

# Remove the individuals and loci determined to have high levels of missing data
struc3 <- struc2 %>%
  dplyr::mutate(
    indiv = as.character(indiv)
  ) %>%
  filter(!indiv %in% is) %>%
  dplyr::select(-c(all_of(l2rem))) %>%
  droplevels()

dim(struc2) # 232 individuals, 869 loci
dim(struc3) # 216 diploid individuals, 561 loci

# Write out a new file as above for the PE libraries, then remove the struc2 and 3 objects

# Create a new Structure file, then proceed with the plan above
# Opened this file periodically in Vim to doublecheck structure
# Used :set list in Vim to see invisible characters
file_nm <- "merged_HWE_missingData_filters.str"

# Remove previous versions
file.remove(file.path(file.path(res_path, "merged"), file_nm))

# Initialize the Structure header, with the two tab symbols at the beginning and a new line symbol at the end
header <- c("\t", "\t", paste(paste(names(struc3)[-grep("^indiv$|^pop$", names(struc3))], collapse = " "), "\n", sep = ""))
# header

# Print the Structure header
cat(header, file = file.path(file.path(res_path, "merged"), file_nm), append = TRUE)

# Print each line of the filtered data frame above
invisible(pblapply(1:nrow(struc3), function(i){

  cat(c(struc3$indiv[i], "\t", struc3$pop[i], "\t", paste(paste(struc3[i, -grep("^indiv$|^pop$", names(struc3))], collapse = " "), "\n", sep = "")), file = file.path(file.path(res_path, "merged"), file_nm), append = TRUE)

}))

# Opened the file in Vim to check out results, looks good
rm(list = c("struc2", "struc3"))

```
